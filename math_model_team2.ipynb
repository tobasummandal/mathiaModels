{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a948650d",
   "metadata": {},
   "source": [
    "# Math Learning Outcome Prediction\n",
    "\n",
    "This notebook predicts math learning outcomes (PostMath scores) using behavioral and learning process variables.\n",
    "\n",
    "## Prediction Variables:\n",
    "- **Pre-test score**: Baseline proficiency predictor\n",
    "- **Error rate**: Total errors / Total attempts (reflects struggle)\n",
    "- **Average error rate**: Errors made / Problems completed\n",
    "- **Hint request rate**: Hints requested / Problems completed\n",
    "- **Total hints requested**: Raw hint usage\n",
    "- **Average help level**: Mean help level (excluding 0)\n",
    "- **Skills mastery rate**: Skills mastered / Skills encountered\n",
    "- **Workspace completion rate**: Graduated workspaces / Total workspaces\n",
    "- **Average workspace duration**: Mean time spent per workspace\n",
    "\n",
    "## Evaluation Metrics:\n",
    "- R-squared (R²)\n",
    "- Root Mean Square Error (RMSE)\n",
    "- Mean Absolute Error (MAE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55fc2c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2652aabe",
   "metadata": {},
   "source": [
    "# Training Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ef32135",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main = pd.read_csv(\"./raw_data/training_set_with_formatted_time.csv\")\n",
    "df_ws = pd.read_csv(\"./raw_data/workspace_summary_train.csv\")\n",
    "df_scores = pd.read_csv(\"./raw_data/student_scores_train.csv\")\n",
    "\n",
    "df_main.drop_duplicates(inplace=True)\n",
    "df_ws.drop_duplicates(inplace=True)\n",
    "df_scores.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d5ad8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary columns\n",
    "df_main.drop(columns=['CF..Anon.School.Id.', 'CF..Anon.Class.Id.', 'Time', 'formatted_time'], inplace=True)\n",
    "\n",
    "# Remove rows containing 'OK_AMBIGUOUS'\n",
    "df_main = df_main[df_main['Outcome'] != 'OK_AMBIGUOUS']\n",
    "\n",
    "df_main.sort_values(by=['Anon.Student.Id', 'datetime'], inplace=True)\n",
    "\n",
    "df_main['datetime'] = pd.to_datetime(\n",
    "    df_main['datetime'],\n",
    "    infer_datetime_format=True,\n",
    "    errors='coerce'       # turns invalid parses into NaT\n",
    ")\n",
    "\n",
    "# Generate time steps\n",
    "df_main['time_step'] = df_main.groupby('Anon.Student.Id')['datetime'].rank(method='first') - 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc313a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main.to_csv('preprocessed_data/df_main_allws.csv', index=False)\n",
    "df_ws.to_csv('preprocessed_data/df_ws_allws.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "792c9b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace_ids_to_remove = [\n",
    "    'worksheet_grapher_a1_lin_mod_mult_rep',\n",
    "    'equation_line_2',\n",
    "    'analyzing_models_2step_rationals',\n",
    "    'multiple_representations_of_linear_functions',\n",
    "    'worksheet_grapher_a1_slope_intercept_integer',\n",
    "    'worksheet_grapher_a1_slope_intercept_decimal',\n",
    "    'connecting_slope_intercept_and_point_slope_forms',\n",
    "    'equation_line_1',\n",
    "    'equation_line_3',\n",
    "    'worksheet_grapher_a1_mod_initial_plus_point',\n",
    "    'worksheet_grapher_a1_mod_two_points',\n",
    "    'modeling_linear_equations_in_standard_form',\n",
    "    'graph_setup_linear_equation-1',\n",
    "    'graph_setup_linear_equation-2',\n",
    "    'classifying_relations_and_functions',\n",
    "    'introduction_to_functions',\n",
    "    'graphs_of_functions',\n",
    "    'graphs_of_functions-1',\n",
    "    'compare_functions_diff_reps_linear_relationships'\n",
    "]\n",
    "\n",
    "\n",
    "df_main = df_main[~df_main['Level..Workspace.Id.'].isin(workspace_ids_to_remove)]\n",
    "df_ws = df_ws[~df_ws['workspace'].isin(workspace_ids_to_remove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5bacddfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned_math = df_scores[[\"Anon.Student.Id\", \"PreMath\", \"PostMath\"]].copy()\n",
    "df_cleaned_math = df_cleaned_math.dropna(subset=[\"PostMath\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2887e04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all to csv and store in processed_data folder\n",
    "df_main.to_csv(\"preprocessed_data/df_main.csv\", index=False)\n",
    "df_ws.to_csv(\"preprocessed_data/df_ws.csv\", index=False)\n",
    "df_scores.to_csv(\"preprocessed_data/df_scores.csv\", index=False)\n",
    "df_cleaned_math.to_csv(\"preprocessed_data/df_cleaned_math.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42443378",
   "metadata": {},
   "source": [
    "# Loading Preprocessed Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0df0a876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Math scores dataset shape: (488, 3)\n",
      "Workspace data shape: (5195, 17)\n",
      "Main interaction data shape: (856606, 13)\n",
      "\n",
      "Datasets loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "print(\"Loading datasets...\")\n",
    "\n",
    "# Load math scores (target variable)\n",
    "math_scores = pd.read_csv('preprocessed_data/df_cleaned_math.csv')\n",
    "print(f\"Math scores dataset shape: {math_scores.shape}\")\n",
    "\n",
    "# Load workspace behavioral data\n",
    "workspace_data = pd.read_csv('preprocessed_data/df_ws.csv')\n",
    "print(f\"Workspace data shape: {workspace_data.shape}\")\n",
    "\n",
    "# Load main interaction data for help levels\n",
    "main_data = pd.read_csv('preprocessed_data/df_main.csv')\n",
    "print(f\"Main interaction data shape: {main_data.shape}\")\n",
    "\n",
    "print(\"\\nDatasets loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8b555ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating behavioral features...\n",
      "Calculating derived behavioral features...\n",
      "Student features shape: (557, 19)\n",
      "Features created: 19 variables\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering: Calculate student-level aggregated features\n",
    "print(\"Creating behavioral features...\")\n",
    "\n",
    "# Group workspace data by student\n",
    "student_features = workspace_data.groupby('Anon.Student.Id').agg({\n",
    "    # Basic counts and totals\n",
    "    'problems_completed': ['sum', 'mean'],\n",
    "    'hint_count': ['sum', 'mean'],\n",
    "    'error_count': ['sum', 'mean'],\n",
    "    'skills_encountered': ['sum', 'mean'],\n",
    "    'skills_mastered': ['sum', 'mean'],\n",
    "    'workspace_total_time_seconds': ['sum', 'mean', 'std'],\n",
    "    'workspace_progress_status': 'count'  # Total workspaces\n",
    "}).round(4)\n",
    "\n",
    "# Flatten column names\n",
    "student_features.columns = ['_'.join(col).strip() for col in student_features.columns]\n",
    "\n",
    "# Calculate derived features\n",
    "print(\"Calculating derived behavioral features...\")\n",
    "\n",
    "# Error rates\n",
    "student_features['total_error_rate'] = (\n",
    "    student_features['error_count_sum'] / \n",
    "    (student_features['error_count_sum'] + student_features['problems_completed_sum'])\n",
    ").fillna(0)\n",
    "\n",
    "student_features['avg_error_rate'] = (\n",
    "    student_features['error_count_sum'] / student_features['problems_completed_sum']\n",
    ").fillna(0)\n",
    "\n",
    "# Hint request rates\n",
    "student_features['hint_request_rate'] = (\n",
    "    student_features['hint_count_sum'] / student_features['problems_completed_sum']\n",
    ").fillna(0)\n",
    "\n",
    "# Skills mastery rate\n",
    "student_features['skills_mastery_rate'] = (\n",
    "    student_features['skills_mastered_sum'] / student_features['skills_encountered_sum']\n",
    ").fillna(0)\n",
    "\n",
    "# Workspace completion analysis\n",
    "workspace_completion = workspace_data.groupby('Anon.Student.Id').agg({\n",
    "    'workspace_progress_status': lambda x: (x == 'GRADUATED').sum(),\n",
    "    'Anon.Student.Id': 'count'\n",
    "})\n",
    "workspace_completion.columns = ['graduated_workspaces', 'total_workspaces']\n",
    "workspace_completion['workspace_completion_rate'] = (\n",
    "    workspace_completion['graduated_workspaces'] / workspace_completion['total_workspaces']\n",
    ")\n",
    "\n",
    "# Merge completion rates\n",
    "student_features = student_features.join(workspace_completion[['workspace_completion_rate']], how='left')\n",
    "\n",
    "print(f\"Student features shape: {student_features.shape}\")\n",
    "print(f\"Features created: {len(student_features.columns)} variables\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a27eae57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating average help level...\n",
      "Help level features added. Shape: (557, 22)\n"
     ]
    }
   ],
   "source": [
    "# Calculate average help level from main interaction data\n",
    "print(\"Calculating average help level...\")\n",
    "\n",
    "# Filter out help level 0 and calculate mean help level per student\n",
    "help_level_data = main_data[main_data['Help.Level'] > 0].groupby('Anon.Student.Id').agg({\n",
    "    'Help.Level': ['mean', 'std', 'count']\n",
    "})\n",
    "help_level_data.columns = ['avg_help_level', 'help_level_std', 'help_requests_count']\n",
    "\n",
    "# Merge with student features\n",
    "student_features = student_features.join(help_level_data, how='left')\n",
    "\n",
    "print(f\"Help level features added. Shape: {student_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d0e1c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating final dataset...\n",
      "Final dataset shape: (488, 25)\n",
      "Students with complete data: 488\n",
      "\n",
      "Available features:\n",
      " 1. PreMath\n",
      " 2. problems_completed_sum\n",
      " 3. problems_completed_mean\n",
      " 4. hint_count_sum\n",
      " 5. hint_count_mean\n",
      " 6. error_count_sum\n",
      " 7. error_count_mean\n",
      " 8. skills_encountered_sum\n",
      " 9. skills_encountered_mean\n",
      "10. skills_mastered_sum\n",
      "11. skills_mastered_mean\n",
      "12. workspace_total_time_seconds_sum\n",
      "13. workspace_total_time_seconds_mean\n",
      "14. workspace_total_time_seconds_std\n",
      "15. workspace_progress_status_count\n",
      "16. total_error_rate\n",
      "17. avg_error_rate\n",
      "18. hint_request_rate\n",
      "19. skills_mastery_rate\n",
      "20. workspace_completion_rate\n",
      "21. avg_help_level\n",
      "22. help_level_std\n",
      "23. help_requests_count\n"
     ]
    }
   ],
   "source": [
    "# Merge with math scores to create final dataset\n",
    "print(\"Creating final dataset...\")\n",
    "\n",
    "# Reset index to make Anon.Student.Id a column\n",
    "student_features_reset = student_features.reset_index()\n",
    "\n",
    "# Merge with math scores\n",
    "final_dataset = math_scores.merge(student_features_reset, on='Anon.Student.Id', how='inner')\n",
    "\n",
    "print(f\"Final dataset shape: {final_dataset.shape}\")\n",
    "print(f\"Students with complete data: {len(final_dataset)}\")\n",
    "\n",
    "# Display the feature names\n",
    "print(\"\\nAvailable features:\")\n",
    "feature_cols = [col for col in final_dataset.columns if col not in ['Anon.Student.Id', 'PostMath']]\n",
    "for i, col in enumerate(feature_cols, 1):\n",
    "    print(f\"{i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "922aa117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for modeling...\n",
      "Feature matrix shape: (488, 23)\n",
      "Target vector shape: (488,)\n",
      "Number of features: 23\n",
      "\n",
      "Warning: Missing values detected. Filling with median...\n",
      "\n",
      "Splitting data into train/test sets...\n",
      "Training set: 390 samples\n",
      "Test set: 98 samples\n",
      "Training target mean: 0.6535\n",
      "Test target mean: 0.6407\n"
     ]
    }
   ],
   "source": [
    "# Prepare features and target for modeling\n",
    "print(\"Preparing data for modeling...\")\n",
    "\n",
    "# Define feature columns (exclude ID and target)\n",
    "feature_columns = [col for col in final_dataset.columns \n",
    "                  if col not in ['Anon.Student.Id', 'PostMath']]\n",
    "\n",
    "X = final_dataset[feature_columns].copy()\n",
    "y = final_dataset['PostMath'].copy()\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target vector shape: {y.shape}\")\n",
    "print(f\"Number of features: {len(feature_columns)}\")\n",
    "\n",
    "# Check for any remaining missing values\n",
    "if X.isnull().sum().sum() > 0:\n",
    "    print(\"\\nWarning: Missing values detected. Filling with median...\")\n",
    "    X = X.fillna(X.median())\n",
    "\n",
    "# Remove infinite values\n",
    "X = X.replace([np.inf, -np.inf], np.nan).fillna(X.median())\n",
    "\n",
    "# Split the data\n",
    "print(\"\\nSplitting data into train/test sets...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=None\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"Training target mean: {y_train.mean():.4f}\")\n",
    "print(f\"Test target mean: {y_test.mean():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "94d3f384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation function defined.\n"
     ]
    }
   ],
   "source": [
    "# Define evaluation function\n",
    "def evaluate_model(model, X_train, y_train, X_test, y_test, model_name):\n",
    "    \"\"\"Evaluate model using R², RMSE, and MAE\"\"\"\n",
    "    \n",
    "    # Make predictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    \n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    \n",
    "    train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "    \n",
    "    # Cross-validation R²\n",
    "    cv_r2 = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')\n",
    "    cv_r2_mean = cv_r2.mean()\n",
    "    cv_r2_std = cv_r2.std()\n",
    "    \n",
    "    results = {\n",
    "        'model': model_name,\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'train_rmse': train_rmse,\n",
    "        'test_rmse': test_rmse,\n",
    "        'train_mae': train_mae,\n",
    "        'test_mae': test_mae,\n",
    "        'cv_r2_mean': cv_r2_mean,\n",
    "        'cv_r2_std': cv_r2_std\n",
    "    }\n",
    "    \n",
    "    return results, y_test_pred\n",
    "\n",
    "print(\"Evaluation function defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "66bca1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Extra Trees...\n",
      "Test R²: 0.8588, Test RMSE: 0.1068, Test MAE: 0.0613\n",
      "\n",
      "All models trained successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize models to test\n",
    "models = {\n",
    "     'Extra Trees': ExtraTreesRegressor(\n",
    "        n_estimators=100, max_depth=10, min_samples_split=5,\n",
    "        min_samples_leaf=2, random_state=42, n_jobs=-1\n",
    "    )\n",
    "}\n",
    "\n",
    "# Train and evaluate all models\n",
    "results_list = []\n",
    "model_predictions = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    results, predictions = evaluate_model(model, X_train, y_train, X_test, y_test, name)\n",
    "    results_list.append(results)\n",
    "    model_predictions[name] = predictions\n",
    "    \n",
    "    print(f\"Test R²: {results['test_r2']:.4f}, Test RMSE: {results['test_rmse']:.4f}, Test MAE: {results['test_mae']:.4f}\")\n",
    "\n",
    "print(\"\\nAll models trained successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "88f7be51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving math model...\n",
      "Math model saved as 'math_model.pkl'\n",
      "Model type: ExtraTreesRegressor\n",
      "Features: 23\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# SAVE THE TRAINED MODEL\n",
    "# ==============================================================================\n",
    "\n",
    "import pickle\n",
    "\n",
    "print(\"Saving math model...\")\n",
    "\n",
    "# Save the model with feature columns\n",
    "with open('math_model.pkl', 'wb') as f:\n",
    "    pickle.dump((model, feature_columns), f)\n",
    "\n",
    "print(\"Math model saved as 'math_model.pkl'\")\n",
    "print(f\"Model type: ExtraTreesRegressor\")\n",
    "print(f\"Features: {len(feature_columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61701d87",
   "metadata": {},
   "source": [
    "# Run Model on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9716d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_main = pd.read_csv('path/to/cleaned/test_df_main.csv') # Replace with path to test time series dataset\n",
    "test_ws = pd.read_csv('path/to/cleaned/test_ws_dataset.csv') # Replace with path to test workspace dataset\n",
    "test_cleaned_math = pd.read_csv('path/to/cleaned/test_cleaned_math.csv') # Replace with path to test metacognition dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9315df5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering: Calculate student-level aggregated features\n",
    "print(\"Creating behavioral features...\")\n",
    "\n",
    "# Group workspace data by student\n",
    "test_student_features = test_ws.groupby('Anon.Student.Id').agg({\n",
    "    # Basic counts and totals\n",
    "    'problems_completed': ['sum', 'mean'],\n",
    "    'hint_count': ['sum', 'mean'],\n",
    "    'error_count': ['sum', 'mean'],\n",
    "    'skills_encountered': ['sum', 'mean'],\n",
    "    'skills_mastered': ['sum', 'mean'],\n",
    "    'workspace_total_time_seconds': ['sum', 'mean', 'std'],\n",
    "    'workspace_progress_status': 'count'  # Total workspaces\n",
    "}).round(4)\n",
    "\n",
    "# Flatten column names\n",
    "test_student_features.columns = ['_'.join(col).strip() for col in student_features.columns]\n",
    "\n",
    "# Calculate derived features\n",
    "print(\"Calculating derived behavioral features...\")\n",
    "\n",
    "# Error rates\n",
    "test_student_features['total_error_rate'] = (\n",
    "    test_student_features['error_count_sum'] / \n",
    "    (test_student_features['error_count_sum'] + test_student_features['problems_completed_sum'])\n",
    ").fillna(0)\n",
    "\n",
    "test_student_features['avg_error_rate'] = (\n",
    "    test_student_features['error_count_sum'] / test_student_features['problems_completed_sum']\n",
    ").fillna(0)\n",
    "\n",
    "# Hint request rates\n",
    "test_student_features['hint_request_rate'] = (\n",
    "    test_student_features['hint_count_sum'] / test_student_features['problems_completed_sum']\n",
    ").fillna(0)\n",
    "\n",
    "# Skills mastery rate\n",
    "test_student_features['skills_mastery_rate'] = (\n",
    "    test_student_features['skills_mastered_sum'] / test_student_features['skills_encountered_sum']\n",
    ").fillna(0)\n",
    "\n",
    "# Workspace completion analysis\n",
    "workspace_completion = test_ws.groupby('Anon.Student.Id').agg({\n",
    "    'workspace_progress_status': lambda x: (x == 'GRADUATED').sum(),\n",
    "    'Anon.Student.Id': 'count'\n",
    "})\n",
    "workspace_completion.columns = ['graduated_workspaces', 'total_workspaces']\n",
    "workspace_completion['workspace_completion_rate'] = (\n",
    "    workspace_completion['graduated_workspaces'] / workspace_completion['total_workspaces']\n",
    ")\n",
    "\n",
    "# Merge completion rates\n",
    "test_student_features = test_student_features.join(workspace_completion[['workspace_completion_rate']], how='left')\n",
    "\n",
    "print(f\"Student features shape: {test_student_features.shape}\")\n",
    "print(f\"Features created: {len(test_student_features.columns)} variables\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086b9377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average help level from main interaction data\n",
    "print(\"Calculating average help level...\")\n",
    "\n",
    "# Filter out help level 0 and calculate mean help level per student\n",
    "help_level_data = test_main[test_main['Help.Level'] > 0].groupby('Anon.Student.Id').agg({\n",
    "    'Help.Level': ['mean', 'std', 'count']\n",
    "})\n",
    "help_level_data.columns = ['avg_help_level', 'help_level_std', 'help_requests_count']\n",
    "\n",
    "# Merge with student features\n",
    "test_student_features = test_student_features.join(help_level_data, how='left')\n",
    "\n",
    "print(f\"Help level features added. Shape: {test_student_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19231847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with test math scores to create final test dataset\n",
    "print(\"Creating final test dataset...\")\n",
    "test_student_features_reset = test_student_features.reset_index()\n",
    "final_test_dataset = test_cleaned_math.merge(test_student_features_reset, on='Anon.Student.Id', how='inner')\n",
    "print(f\"Final test dataset shape: {final_test_dataset.shape}\")\n",
    "\n",
    "# Prepare features for prediction\n",
    "print(\"Preparing test data for prediction...\")\n",
    "feature_columns = [col for col in final_test_dataset.columns \n",
    "                  if col not in ['Anon.Student.Id', 'PostMath']]\n",
    "X_test_new = final_test_dataset[feature_columns].copy()\n",
    "\n",
    "# Handle missing values and infinite values in test data (consistent with training)\n",
    "if X_test_new.isnull().sum().sum() > 0:\n",
    "    print(\"Warning: Missing values detected in test data. Filling with median...\")\n",
    "    X_test_new = X_test_new.fillna(X_test_new.median())\n",
    "X_test_new = X_test_new.replace([np.inf, -np.inf], np.nan).fillna(X_test_new.median())\n",
    "\n",
    "print(f\"Test feature matrix shape: {X_test_new.shape}\")\n",
    "\n",
    "# Make predictions using the trained Extra Trees model\n",
    "print(\"Making predictions on the test dataset...\")\n",
    "extra_trees_model = models['Extra Trees']\n",
    "test_predictions = extra_trees_model.predict(X_test_new)\n",
    "\n",
    "# Save predictions to a CSV file\n",
    "predictions_df = pd.DataFrame({'Anon.Student.Id': final_test_dataset['Anon.Student.Id'], 'PredictedPostMath': test_predictions})\n",
    "predictions_df.to_csv('test_predictions.csv', index=False)\n",
    "print(\"Predictions saved to test_predictions.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mathia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
