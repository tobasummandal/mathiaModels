{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b084b623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning imports - CLASSIFICATION ONLY\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, cohen_kappa_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import warnings\n",
    "import pickle\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"MOTIVATION PREDICTION MODEL\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Predicting Motivation scores using behavioral features\")\n",
    "print(\"Target: Motivation (Average of PostSE and PostMAP)\")\n",
    "print(\"Features: MAP-specific and SE-specific behavioral patterns\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33b96b7",
   "metadata": {},
   "source": [
    "# Test Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba5811f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPLACE WITH TEST DATASET FILE PATHS\n",
    "df_main = pd.read_csv(\"./raw_data/training_set_with_formatted_time.csv\")\n",
    "df_ws = pd.read_csv(\"./raw_data/workspace_summary_train.csv\")\n",
    "df_scores = pd.read_csv(\"./raw_data/student_scores_train.csv\")\n",
    "\n",
    "df_main.drop_duplicates(inplace=True)\n",
    "df_ws.drop_duplicates(inplace=True)\n",
    "df_scores.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682b5f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary columns\n",
    "df_main.drop(columns=['CF..Anon.School.Id.', 'CF..Anon.Class.Id.', 'Time', 'formatted_time'], inplace=True)\n",
    "\n",
    "# Remove rows containing 'OK_AMBIGUOUS'\n",
    "df_main = df_main[df_main['Outcome'] != 'OK_AMBIGUOUS']\n",
    "\n",
    "df_main.sort_values(by=['Anon.Student.Id', 'datetime'], inplace=True)\n",
    "\n",
    "df_main['datetime'] = pd.to_datetime(\n",
    "    df_main['datetime'],\n",
    "    infer_datetime_format=True,\n",
    "    errors='coerce'       # turns invalid parses into NaT\n",
    ")\n",
    "\n",
    "# Generate time steps\n",
    "df_main['time_step'] = df_main.groupby('Anon.Student.Id')['datetime'].rank(method='first') - 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee484e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPLACE WITH TEST DESIRED FILE PATHS\n",
    "df_main.to_csv('preprocessed_data/df_main_allws.csv', index=False)\n",
    "df_ws.to_csv('preprocessed_data/df_ws_allws.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64701832",
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace_ids_to_remove = [\n",
    "    'worksheet_grapher_a1_lin_mod_mult_rep',\n",
    "    'equation_line_2',\n",
    "    'analyzing_models_2step_rationals',\n",
    "    'multiple_representations_of_linear_functions',\n",
    "    'worksheet_grapher_a1_slope_intercept_integer',\n",
    "    'worksheet_grapher_a1_slope_intercept_decimal',\n",
    "    'connecting_slope_intercept_and_point_slope_forms',\n",
    "    'equation_line_1',\n",
    "    'equation_line_3',\n",
    "    'worksheet_grapher_a1_mod_initial_plus_point',\n",
    "    'worksheet_grapher_a1_mod_two_points',\n",
    "    'modeling_linear_equations_in_standard_form',\n",
    "    'graph_setup_linear_equation-1',\n",
    "    'graph_setup_linear_equation-2',\n",
    "    'classifying_relations_and_functions',\n",
    "    'introduction_to_functions',\n",
    "    'graphs_of_functions',\n",
    "    'graphs_of_functions-1',\n",
    "    'compare_functions_diff_reps_linear_relationships'\n",
    "]\n",
    "\n",
    "\n",
    "df_main = df_main[~df_main['Level..Workspace.Id.'].isin(workspace_ids_to_remove)]\n",
    "df_ws = df_ws[~df_ws['workspace'].isin(workspace_ids_to_remove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c5dcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPLACE WITH DESIRED OUTPUT DIRECTORY\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "# For MAP: Use average of Post1MAP and Post2MAP for Post3MAP\n",
    "df_cleaned_map = df_scores[[\"Anon.Student.Id\", \"PreMAP\", \"Post1MAP\", \"Post2MAP\"]].copy()\n",
    "df_cleaned_map[[\"PreMAP\", \"Post1MAP\", \"Post2MAP\"]] = imputer.fit_transform(df_cleaned_map[[\"PreMAP\", \"Post1MAP\", \"Post2MAP\"]])\n",
    "# Calculate Post3MAP as average of Post1MAP and Post2MAP\n",
    "df_cleaned_map['Post3MAP'] = (df_cleaned_map['Post1MAP'] + df_cleaned_map['Post2MAP']) / 2\n",
    "\n",
    "# For SE: Use average of Post1SE and Post2SE for Post3SE\n",
    "df_cleaned_se = df_scores[[\"Anon.Student.Id\", \"PreSE\", \"Post1SE\", \"Post2SE\"]].copy()\n",
    "df_cleaned_se[[\"PreSE\", \"Post1SE\", \"Post2SE\"]] = imputer.fit_transform(df_cleaned_se[[\"PreSE\", \"Post1SE\", \"Post2SE\"]])\n",
    "# Calculate Post3SE as average of Post1SE and Post2SE\n",
    "df_cleaned_se['Post3SE'] = (df_cleaned_se['Post1SE'] + df_cleaned_se['Post2SE']) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca8ba0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPLACE WITH DESIRED OUTPUT DIRECTORY\n",
    "df_main.to_csv(\"preprocessed_data/df_main.csv\", index=False)\n",
    "df_ws.to_csv(\"preprocessed_data/df_ws.csv\", index=False)\n",
    "df_scores.to_csv(\"preprocessed_data/df_scores.csv\", index=False)\n",
    "df_cleaned_map.to_csv(\"preprocessed_data/df_cleaned_map.csv\", index=False)\n",
    "df_cleaned_se.to_csv(\"preprocessed_data/df_cleaned_se.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0905166",
   "metadata": {},
   "source": [
    "# Loading Preprocessed Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90c0b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# DATA LOADING AND VALIDATION\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\nLOADING AND VALIDATING DATA\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Load all data files\n",
    "df_main = pd.read_csv('preprocessed_data/df_main_allws.csv')\n",
    "df_ws = pd.read_csv('preprocessed_data/df_ws_allws.csv')\n",
    "df_cleaned_map = pd.read_csv('preprocessed_data/df_cleaned_map.csv')\n",
    "df_cleaned_se = pd.read_csv('preprocessed_data/df_cleaned_se.csv')\n",
    "\n",
    "print(f\"✓ Main interaction data: {len(df_main):,} records\")\n",
    "print(f\"✓ Workspace data: {len(df_ws):,} workspace sessions\")\n",
    "print(f\"✓ MAP assessments: {len(df_cleaned_map):,} students\")\n",
    "print(f\"✓ SE assessments: {len(df_cleaned_se):,} students\")\n",
    "\n",
    "print(f\"\\nVALIDATING MOTIVATION DATA:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"MAP columns:\", df_cleaned_map.columns.tolist())\n",
    "print(\"SE columns:\", df_cleaned_se.columns.tolist())\n",
    "\n",
    "# Check for Post3SE and Post3MAP specifically\n",
    "missing_cols = []\n",
    "\n",
    "if 'Post3MAP' not in df_cleaned_map.columns:\n",
    "    missing_cols.append('Post3MAP')\n",
    "if 'Post3SE' not in df_cleaned_se.columns:\n",
    "    missing_cols.append('Post3SE')\n",
    "\n",
    "if missing_cols:\n",
    "    print(f\"ERROR: Missing columns: {missing_cols}\")\n",
    "    print(\"Available MAP columns:\", df_cleaned_map.columns.tolist())\n",
    "    print(\"Available SE columns:\", df_cleaned_se.columns.tolist())\n",
    "else:\n",
    "    print(\"Post3MAP found in MAP data\")\n",
    "    non_null_count_map = df_cleaned_map['Post3MAP'].notna().sum()\n",
    "    col_range_map = f\"{df_cleaned_map['Post3MAP'].min():.2f} to {df_cleaned_map['Post3MAP'].max():.2f}\"\n",
    "    print(f\"  - Non-null values: {non_null_count_map}, Range: {col_range_map}\")\n",
    "    \n",
    "    print(\"Post3SE found in SE data\")\n",
    "    non_null_count_se = df_cleaned_se['Post3SE'].notna().sum()\n",
    "    col_range_se = f\"{df_cleaned_se['Post3SE'].min():.2f} to {df_cleaned_se['Post3SE'].max():.2f}\"\n",
    "    print(f\"  - Non-null values: {non_null_count_se}, Range: {col_range_se}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d1bf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TARGET VARIABLE CREATION - MOTIVATION SCORES\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\nCREATING MOTIVATION TARGET VARIABLE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Merge the two datasets on student ID\n",
    "df_combined = df_cleaned_se.merge(df_cleaned_map, on='Anon.Student.Id', how='inner')\n",
    "\n",
    "# Filter students with both PostSE and PostMAP scores\n",
    "motivation_data = df_combined[\n",
    "    df_combined['Post3SE'].notna() & \n",
    "    df_combined['Post3MAP'].notna()\n",
    "].copy()\n",
    "\n",
    "\n",
    "print(f\"Students with both PostSE and PostMAP scores: {len(motivation_data)}\")\n",
    "\n",
    "# Create motivation score as average of PostSE and PostMAP\n",
    "motivation_data['motivation_score'] = (motivation_data['Post3SE'] + motivation_data['Post3MAP']) / 2\n",
    "\n",
    "# Create categorical version (Low, Medium, High)\n",
    "def categorize_motivation(score):\n",
    "    if score <= 4.0:\n",
    "        return 'Low'\n",
    "    elif score <= 5.5:\n",
    "        return 'Medium'\n",
    "    else:\n",
    "        return 'High'\n",
    "\n",
    "motivation_data['motivation_category'] = motivation_data['motivation_score'].apply(categorize_motivation)\n",
    "\n",
    "# Show distribution\n",
    "print(f\"\\nMotivation score statistics:\")\n",
    "print(f\"  Mean: {motivation_data['motivation_score'].mean():.2f}\")\n",
    "print(f\"  Std: {motivation_data['motivation_score'].std():.2f}\")\n",
    "print(f\"  Range: {motivation_data['motivation_score'].min():.2f} - {motivation_data['motivation_score'].max():.2f}\")\n",
    "\n",
    "print(f\"\\nIndividual component statistics:\")\n",
    "print(f\"  Post3SE - Mean: {motivation_data['Post3SE'].mean():.2f}, Std: {motivation_data['Post3SE'].std():.2f}\")\n",
    "print(f\"  Post3MAP - Mean: {motivation_data['Post3MAP'].mean():.2f}, Std: {motivation_data['Post3MAP'].std():.2f}\")\n",
    "\n",
    "category_counts = motivation_data['motivation_category'].value_counts()\n",
    "print(f\"\\nMotivation category distribution:\")\n",
    "for category, count in category_counts.items():\n",
    "    percentage = (count / len(motivation_data)) * 100\n",
    "    print(f\"  • {category}: {count} students ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTarget variable created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93904bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# FEATURE ENGINEERING - MAP & SE FOCUSED FEATURES\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\nENGINEERING MAP & SE FOCUSED FEATURES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get students with motivation scores\n",
    "target_students = motivation_data['Anon.Student.Id'].unique()\n",
    "df_filtered = df_main[df_main['Anon.Student.Id'].isin(target_students)].copy()\n",
    "df_ws_filtered = df_ws[df_ws['Anon.Student.Id'].isin(target_students)].copy()\n",
    "\n",
    "print(f\"Filtering data for {len(target_students)} students with motivation scores\")\n",
    "print(f\"Behavioral data: {len(df_filtered):,} interactions\")\n",
    "print(f\"Workspace data: {len(df_ws_filtered):,} workspace sessions\")\n",
    "\n",
    "# Feature engineering by student\n",
    "features_list = []\n",
    "\n",
    "for student_id in target_students:\n",
    "    student_data = df_filtered[df_filtered['Anon.Student.Id'] == student_id].copy()\n",
    "    student_ws = df_ws_filtered[df_ws_filtered['Anon.Student.Id'] == student_id].copy()\n",
    "    \n",
    "    if len(student_data) == 0:\n",
    "        continue\n",
    "    \n",
    "    features = {'Anon.Student.Id': student_id}\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # MAP-RELATED FEATURES (Mastery-Oriented Behavior)\n",
    "    # ==========================================================================\n",
    "    \n",
    "    # 1. Sessions per Student, balanced by average problems solved per session\n",
    "    unique_sessions = student_data['Session.Id'].nunique()\n",
    "    total_problems = student_data['Problem.Name'].nunique()\n",
    "    avg_problems_per_session = total_problems / unique_sessions if unique_sessions > 0 else 0\n",
    "    features['sessions_balanced_by_problems'] = unique_sessions * (avg_problems_per_session / 10)  # Normalize\n",
    "    \n",
    "    # 2. Number of Problems Attempted\n",
    "    features['problems_attempted'] = total_problems\n",
    "    \n",
    "    # 3. Percentage of Graduated Workspaces\n",
    "    if len(student_ws) > 0:\n",
    "        graduated_ws = student_ws['Graduate'].sum() if 'Graduate' in student_ws.columns else 0\n",
    "        total_ws = len(student_ws)\n",
    "        features['graduated_workspaces_pct'] = graduated_ws / total_ws if total_ws > 0 else 0\n",
    "    else:\n",
    "        features['graduated_workspaces_pct'] = 0\n",
    "    \n",
    "    # 4. Hint Usage Pattern (strategic use of hints)\n",
    "    hint_followed_by_success = 0\n",
    "    total_hint_instances = 0\n",
    "    \n",
    "    for problem in student_data['Problem.Name'].unique():\n",
    "        problem_data = student_data[student_data['Problem.Name'] == problem].sort_values('time_step')\n",
    "        \n",
    "        for i in range(len(problem_data) - 1):\n",
    "            current_action = str(problem_data.iloc[i]['Action']).lower()\n",
    "            next_outcome = str(problem_data.iloc[i + 1]['Outcome']).upper()\n",
    "            \n",
    "            if 'hint' in current_action:\n",
    "                total_hint_instances += 1\n",
    "                if next_outcome in ['OK', 'CORRECT']:\n",
    "                    hint_followed_by_success += 1\n",
    "    \n",
    "    features['hint_usage_pattern'] = hint_followed_by_success / total_hint_instances if total_hint_instances > 0 else 0\n",
    "    \n",
    "    # 5. Change in Help Level Over Time (learning progression)\n",
    "    if 'Help.Level' in student_data.columns:\n",
    "        help_levels = student_data['Help.Level'].dropna()\n",
    "        if len(help_levels) > 1:\n",
    "            # Calculate trend: negative means decreasing help needs over time\n",
    "            x = np.arange(len(help_levels))\n",
    "            slope = np.polyfit(x, help_levels, 1)[0]\n",
    "            features['help_level_change'] = -slope  # Negative slope is good (less help over time)\n",
    "        else:\n",
    "            features['help_level_change'] = 0\n",
    "    else:\n",
    "        features['help_level_change'] = 0\n",
    "    \n",
    "    # 6. Step Completion Rate\n",
    "    completed_steps = student_data['Outcome'].isin(['OK', 'CORRECT']).sum()\n",
    "    total_steps = len(student_data)\n",
    "    features['step_completion_rate'] = completed_steps / total_steps if total_steps > 0 else 0\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # SE-RELATED FEATURES (Self-Efficacy)\n",
    "    # ==========================================================================\n",
    "    \n",
    "    # 7. Ratio of Attempts to Hint Requests\n",
    "    attempt_actions = student_data['Action'].str.contains('attempt|submit', case=False, na=False).sum()\n",
    "    hint_actions = student_data['Action'].str.contains('hint', case=False, na=False).sum()\n",
    "    features['attempt_to_hint_ratio'] = attempt_actions / hint_actions if hint_actions > 0 else attempt_actions\n",
    "    \n",
    "    # 8. Quick Return After Error (resilience)\n",
    "    quick_returns = 0\n",
    "    error_instances = 0\n",
    "    \n",
    "    for i in range(len(student_data) - 1):\n",
    "        if student_data.iloc[i]['Outcome'] in ['ERROR', 'WRONG']:\n",
    "            error_instances += 1\n",
    "            time_diff = student_data.iloc[i + 1]['time_step'] - student_data.iloc[i]['time_step']\n",
    "            if time_diff < 30:  # Quick return within 30 seconds\n",
    "                quick_returns += 1\n",
    "    \n",
    "    features['quick_return_rate'] = quick_returns / error_instances if error_instances > 0 else 1\n",
    "    \n",
    "    # 9. Average Help Level (lower = more self-efficacy)\n",
    "    if 'Help.Level' in student_data.columns:\n",
    "        help_levels = student_data['Help.Level'].dropna()\n",
    "        help_levels_nonzero = help_levels[help_levels > 0]\n",
    "        features['avg_help_level'] = help_levels_nonzero.mean() if len(help_levels_nonzero) > 0 else 0\n",
    "    else:\n",
    "        features['avg_help_level'] = 0\n",
    "    \n",
    "    # 10. Proportion of Low-Level Hints (SE indicator)\n",
    "    if 'Help.Level' in student_data.columns:\n",
    "        hint_requests = student_data[student_data['Action'].str.contains('hint', case=False, na=False)]\n",
    "        if len(hint_requests) > 0:\n",
    "            low_level_hints = hint_requests['Help.Level'].isin([0, 1]).sum()\n",
    "            features['low_level_hints_prop'] = low_level_hints / len(hint_requests)\n",
    "        else:\n",
    "            features['low_level_hints_prop'] = 0\n",
    "    else:\n",
    "        features['low_level_hints_prop'] = 0\n",
    "    \n",
    "    # 11. Proportion of High-Level Hints (lower SE indicator)\n",
    "    if 'Help.Level' in student_data.columns:\n",
    "        hint_requests = student_data[student_data['Action'].str.contains('hint', case=False, na=False)]\n",
    "        if len(hint_requests) > 0:\n",
    "            high_level_hints = hint_requests['Help.Level'].isin([4, 5]).sum()\n",
    "            features['high_level_hints_prop'] = high_level_hints / len(hint_requests)\n",
    "        else:\n",
    "            features['high_level_hints_prop'] = 0\n",
    "    else:\n",
    "        features['high_level_hints_prop'] = 0\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # ADDITIONAL MOTIVATION-RELATED FEATURES\n",
    "    # ==========================================================================\n",
    "    \n",
    "    # 12. Session Persistence\n",
    "    features['session_persistence'] = unique_sessions\n",
    "    \n",
    "    # 13. Total Interaction Time\n",
    "    if len(student_data) > 1:\n",
    "        features['total_interaction_time'] = student_data['time_step'].max() - student_data['time_step'].min()\n",
    "    else:\n",
    "        features['total_interaction_time'] = 0\n",
    "    \n",
    "    # 14. Problem-solving Efficiency\n",
    "    avg_time_per_problem = features['total_interaction_time'] / total_problems if total_problems > 0 else 0\n",
    "    features['problem_solving_efficiency'] = 1 / (1 + avg_time_per_problem / 60)  # Normalize to minutes\n",
    "    \n",
    "    features_list.append(features)\n",
    "\n",
    "# Create features dataframe\n",
    "features_df = pd.DataFrame(features_list)\n",
    "print(f\"\\nMotivation features engineered for {len(features_df)} students\")\n",
    "print(f\"Total features: {len(features_df.columns) - 1}\")\n",
    "\n",
    "print(\"\\nMAP-FOCUSED FEATURES:\")\n",
    "print(\"-\" * 30)\n",
    "map_features = [\n",
    "    'sessions_balanced_by_problems', 'problems_attempted', 'graduated_workspaces_pct',\n",
    "    'hint_usage_pattern', 'help_level_change', 'step_completion_rate'\n",
    "]\n",
    "\n",
    "for feat in map_features:\n",
    "    if feat in features_df.columns:\n",
    "        mean_val = features_df[feat].mean()\n",
    "        print(f\"  ✓ {feat}: mean = {mean_val:.3f}\")\n",
    "\n",
    "print(\"\\nSE-FOCUSED FEATURES:\")\n",
    "print(\"-\" * 30)\n",
    "se_features = [\n",
    "    'attempt_to_hint_ratio', 'quick_return_rate', 'avg_help_level',\n",
    "    'low_level_hints_prop', 'high_level_hints_prop'\n",
    "]\n",
    "\n",
    "for feat in se_features:\n",
    "    if feat in features_df.columns:\n",
    "        mean_val = features_df[feat].mean()\n",
    "        print(f\"  ✓ {feat}: mean = {mean_val:.3f}\")\n",
    "\n",
    "# Display feature statistics\n",
    "print(f\"\\nFeature summary:\")\n",
    "print(features_df.describe().round(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23edc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# DATA MERGING AND PREPARATION\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\nMERGING FEATURES WITH TARGET VARIABLE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Merge features with motivation scores\n",
    "df = features_df.merge(\n",
    "    motivation_data[['Anon.Student.Id', 'motivation_score', 'motivation_category', 'Post3SE', 'Post3MAP']], \n",
    "    on='Anon.Student.Id', how='inner'\n",
    ")\n",
    "\n",
    "print(f\"Final dataset: {len(df)} students\")\n",
    "print(f\"Features: {len(df.columns) - 5}\")  # Excluding ID and target columns\n",
    "\n",
    "# Handle missing values\n",
    "print(f\"\\nMissing values per column:\")\n",
    "missing_counts = df.isnull().sum()\n",
    "for col, count in missing_counts.items():\n",
    "    if count > 0:\n",
    "        print(f\"  {col}: {count}\")\n",
    "\n",
    "# Fill missing values with median for numerical columns\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "numerical_cols = [col for col in numerical_cols if col not in ['motivation_score', 'PostSE', 'PostMAP']]\n",
    "\n",
    "postse_median = df['Post3SE'].median()\n",
    "postmap_median = df['Post3MAP'].median()\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "df[numerical_cols] = imputer.fit_transform(df[numerical_cols])\n",
    "\n",
    "print(f\"\\nData preparation complete!\")\n",
    "print(f\"Shape: {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c6b35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# QWK METRIC DEFINITION\n",
    "# ==============================================================================\n",
    "\n",
    "def qwk(y_true, y_pred):\n",
    "    \"\"\"Quadratic Weighted Kappa - primary metric for ordinal classification\"\"\"\n",
    "    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
    "\n",
    "def qwk_scorer(estimator, X, y):\n",
    "    \"\"\"Custom scorer for cross-validation\"\"\"\n",
    "    return qwk(y, estimator.predict(X))\n",
    "\n",
    "print(\"QWK metric defined for ordinal classification evaluation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41a0c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# LOAD TRAINED MODEL AND MAKE PREDICTIONS\n",
    "# ==============================================================================\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"Loading trained motivation model...\")\n",
    "\n",
    "# Load the saved model and feature columns\n",
    "with open('motivation_model.pkl', 'rb') as f:\n",
    "    model, feature_cols = pickle.load(f)\n",
    "\n",
    "print(f\"✓ Model loaded successfully!\")\n",
    "print(f\"✓ Model type: {type(model).__name__}\")\n",
    "print(f\"✓ Number of features: {len(feature_cols)}\")\n",
    "\n",
    "# Prepare features for prediction\n",
    "print(\"\\nPreparing features for prediction...\")\n",
    "\n",
    "# Select only the features used during training (excluding target variables)\n",
    "X_test = df[feature_cols].copy()\n",
    "\n",
    "# Handle missing values (same as training)\n",
    "print(\"Handling missing values...\")\n",
    "X_test = X_test.replace([np.inf, -np.inf], np.nan)\n",
    "X_test = X_test.fillna(0)  # Fill with 0 for missing values\n",
    "\n",
    "print(f\"✓ Test features shape: {X_test.shape}\")\n",
    "print(f\"✓ Features with missing values: {X_test.isnull().sum().sum()}\")\n",
    "\n",
    "# Make predictions\n",
    "print(\"\\nMaking predictions...\")\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "print(f\"✓ Predictions made for {len(predictions)} students\")\n",
    "print(f\"✓ Prediction range: {predictions.min():.2f} to {predictions.max():.2f}\")\n",
    "\n",
    "# Create results dataframe\n",
    "results_df = pd.DataFrame({\n",
    "    'Anon.Student.Id': df['Anon.Student.Id'],\n",
    "    'PostMotivation3_Predicted': predictions\n",
    "})\n",
    "\n",
    "print(f\"\\nResults shape: {results_df.shape}\")\n",
    "print(\"\\nFirst 10 predictions:\")\n",
    "print(results_df.head(10))\n",
    "\n",
    "# Save predictions\n",
    "output_file = 'motivation_predictions.csv'\n",
    "results_df.to_csv(output_file, index=False)\n",
    "print(f\"\\n✓ Predictions saved to '{output_file}'\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
