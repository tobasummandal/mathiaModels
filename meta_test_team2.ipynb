{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e091b495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning imports - CLASSIFICATION ONLY\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, cohen_kappa_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import warnings\n",
    "import pickle\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"METACOGNITION PREDICTION MODEL\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Predicting Post3Meta scores using behavioral features\")\n",
    "print(\"Target: Post3Meta (Metacognition Assessment)\")\n",
    "print(\"Features: Learning behavior patterns and help-seeking\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3bc57f",
   "metadata": {},
   "source": [
    "# Test Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c13a3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPLACE WITH TEST DATASET FILE PATHS\n",
    "df_main = pd.read_csv(\"./raw_data/training_set_with_formatted_time.csv\")\n",
    "df_ws = pd.read_csv(\"./raw_data/workspace_summary_train.csv\")\n",
    "df_scores = pd.read_csv(\"./raw_data/student_scores_train.csv\")\n",
    "\n",
    "df_main.drop_duplicates(inplace=True)\n",
    "df_ws.drop_duplicates(inplace=True)\n",
    "df_scores.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace25262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary columns\n",
    "df_main.drop(columns=['CF..Anon.School.Id.', 'CF..Anon.Class.Id.', 'Time', 'formatted_time'], inplace=True)\n",
    "\n",
    "# Remove rows containing 'OK_AMBIGUOUS'\n",
    "df_main = df_main[df_main['Outcome'] != 'OK_AMBIGUOUS']\n",
    "\n",
    "df_main.sort_values(by=['Anon.Student.Id', 'datetime'], inplace=True)\n",
    "\n",
    "df_main['datetime'] = pd.to_datetime(\n",
    "    df_main['datetime'],\n",
    "    infer_datetime_format=True,\n",
    "    errors='coerce'       # turns invalid parses into NaT\n",
    ")\n",
    "\n",
    "# Generate time steps\n",
    "df_main['time_step'] = df_main.groupby('Anon.Student.Id')['datetime'].rank(method='first') - 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a00a213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPLACE WITH TEST DESIRED FILE PATHS\n",
    "df_main.to_csv('preprocessed_data/df_main_allws.csv', index=False)\n",
    "df_ws.to_csv('preprocessed_data/df_ws_allws.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e560ffd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace_ids_to_remove = [\n",
    "    'worksheet_grapher_a1_lin_mod_mult_rep',\n",
    "    'equation_line_2',\n",
    "    'analyzing_models_2step_rationals',\n",
    "    'multiple_representations_of_linear_functions',\n",
    "    'worksheet_grapher_a1_slope_intercept_integer',\n",
    "    'worksheet_grapher_a1_slope_intercept_decimal',\n",
    "    'connecting_slope_intercept_and_point_slope_forms',\n",
    "    'equation_line_1',\n",
    "    'equation_line_3',\n",
    "    'worksheet_grapher_a1_mod_initial_plus_point',\n",
    "    'worksheet_grapher_a1_mod_two_points',\n",
    "    'modeling_linear_equations_in_standard_form',\n",
    "    'graph_setup_linear_equation-1',\n",
    "    'graph_setup_linear_equation-2',\n",
    "    'classifying_relations_and_functions',\n",
    "    'introduction_to_functions',\n",
    "    'graphs_of_functions',\n",
    "    'graphs_of_functions-1',\n",
    "    'compare_functions_diff_reps_linear_relationships'\n",
    "]\n",
    "\n",
    "\n",
    "df_main = df_main[~df_main['Level..Workspace.Id.'].isin(workspace_ids_to_remove)]\n",
    "df_ws = df_ws[~df_ws['workspace'].isin(workspace_ids_to_remove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8400fb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPLACE WITH TEST DESIRED FILE PATHS AND NAMES\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "df_cleaned_meta = df_scores[[\"Anon.Student.Id\", \"PreMeta\", \"Post1Meta\", \"Post2Meta\"]].copy()\n",
    "df_cleaned_meta[[\"PreMeta\", \"Post1Meta\", \"Post2Meta\"]] = imputer.fit_transform(df_cleaned_meta[[\"PreMeta\", \"Post1Meta\", \"Post2Meta\"]])\n",
    "\n",
    "# Create average of Post1Meta and Post2Meta\n",
    "df_cleaned_meta['Post1Post2_Avg'] = df_cleaned_meta[['Post1Meta', 'Post2Meta']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5767f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all to csv and store in processed_data folder\n",
    "df_main.to_csv(\"preprocessed_data/df_main.csv\", index=False)\n",
    "df_ws.to_csv(\"preprocessed_data/df_ws.csv\", index=False)\n",
    "df_scores.to_csv(\"preprocessed_data/df_scores.csv\", index=False)\n",
    "df_cleaned_meta.to_csv(\"preprocessed_data/df_cleaned_meta.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ad7915",
   "metadata": {},
   "source": [
    "# Loading Preprocessed Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8e1c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# DATA LOADING AND VALIDATION\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\nLOADING AND VALIDATING DATA\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Load all data files\n",
    "df_main = pd.read_csv('preprocessed_data/df_main_allws.csv')\n",
    "df_ws = pd.read_csv('preprocessed_data/df_ws_allws.csv')\n",
    "df_cleaned_meta = pd.read_csv('preprocessed_data/df_cleaned_meta.csv')\n",
    "\n",
    "print(f\"✓ Main interaction data: {len(df_main):,} records\")\n",
    "print(f\"✓ Workspace data: {len(df_ws):,} workspace sessions\")\n",
    "print(f\"✓ Metacognition assessments: {len(df_cleaned_meta):,} students\")\n",
    "\n",
    "print(f\"\\nVALIDATING METACOGNITION DATA:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"Meta columns:\", df_cleaned_meta.columns.tolist())\n",
    "\n",
    "# Check for Post1Post2_Avg specifically\n",
    "if 'Post1Post2_Avg' not in df_cleaned_meta.columns:\n",
    "    print(\"ERROR: Post1Post2_Avg column not found!\")\n",
    "    print(\"Available Meta columns:\", [col for col in df_cleaned_meta.columns if 'Meta' in col or 'Avg' in col])\n",
    "else:\n",
    "    print(f\"✓ Post1Post2_Avg found: {df_cleaned_meta['Post1Post2_Avg'].notna().sum()} non-null values\")\n",
    "    print(f\"  Range: {df_cleaned_meta['Post1Post2_Avg'].min():.2f} to {df_cleaned_meta['Post1Post2_Avg'].max():.2f}\")\n",
    "    print(f\"  Post1Meta mean: {df_cleaned_meta['Post1Meta'].mean():.2f}\")\n",
    "    print(f\"  Post2Meta mean: {df_cleaned_meta['Post2Meta'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e52c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TARGET VARIABLE CREATION\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\nCREATING TARGET VARIABLE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Filter students with Post1Post2_Avg scores\n",
    "metacognition_data = df_cleaned_meta[df_cleaned_meta['PreMeta'].notna() & df_cleaned_meta['Post1Post2_Avg'].notna()].copy()\n",
    "print(f\"Students with Post1Post2_Avg scores: {len(metacognition_data)}\")\n",
    "\n",
    "# Create both continuous and categorical targets\n",
    "metacognition_data['metacognition_score'] = metacognition_data['Post1Post2_Avg']\n",
    "\n",
    "# Create categorical version (Low, Medium, High)\n",
    "def categorize_metacognition(score):\n",
    "    if score <= 4.0:\n",
    "        return 'Low'\n",
    "    elif score <= 5.5:\n",
    "        return 'Medium'\n",
    "    else:\n",
    "        return 'High'\n",
    "\n",
    "metacognition_data['metacognition_category'] = metacognition_data['metacognition_score'].apply(categorize_metacognition)\n",
    "\n",
    "# Show distribution\n",
    "print(f\"\\nMetacognition score statistics:\")\n",
    "print(f\"  Mean: {metacognition_data['metacognition_score'].mean():.2f}\")\n",
    "print(f\"  Std: {metacognition_data['metacognition_score'].std():.2f}\")\n",
    "print(f\"  Range: {metacognition_data['metacognition_score'].min():.2f} - {metacognition_data['metacognition_score'].max():.2f}\")\n",
    "\n",
    "category_counts = metacognition_data['metacognition_category'].value_counts()\n",
    "print(f\"\\nMetacognition category distribution:\")\n",
    "for category, count in category_counts.items():\n",
    "    percentage = (count / len(metacognition_data)) * 100\n",
    "    print(f\"  • {category}: {count} students ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTarget variable created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d286891a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# FEATURE ENGINEERING - METACOGNITION-FOCUSED FEATURES\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\nENGINEERING METACOGNITION-FOCUSED FEATURES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get students with metacognition scores\n",
    "target_students = metacognition_data['Anon.Student.Id'].unique()\n",
    "df_filtered = df_main[df_main['Anon.Student.Id'].isin(target_students)].copy()\n",
    "\n",
    "print(f\"Filtering data for {len(target_students)} students with metacognition scores\")\n",
    "print(f\"Behavioral data: {len(df_filtered):,} interactions\")\n",
    "\n",
    "# Feature engineering by student\n",
    "features_list = []\n",
    "\n",
    "for student_id in target_students:\n",
    "    student_data = df_filtered[df_filtered['Anon.Student.Id'] == student_id].copy()\n",
    "    \n",
    "    if len(student_data) == 0:\n",
    "        continue\n",
    "    \n",
    "    features = {'Anon.Student.Id': student_id}\n",
    "    \n",
    "    # 1. Average Help Level (exclude 0 if 0 = no hint)\n",
    "    if 'Help.Level' in student_data.columns:\n",
    "        help_levels = student_data['Help.Level'].dropna()\n",
    "        help_levels_nonzero = help_levels[help_levels > 0]  # Exclude 0 if 0 = no hint\n",
    "        features['avg_help_level'] = help_levels_nonzero.mean() if len(help_levels_nonzero) > 0 else 0\n",
    "    else:\n",
    "        features['avg_help_level'] = 0\n",
    "    \n",
    "    # 2. Hint Usage Pattern (% of problems with Hint Requested followed by OK/WRONG)\n",
    "    hint_pattern_count = 0\n",
    "    total_problems = student_data['Problem.Name'].nunique()\n",
    "    \n",
    "    for problem in student_data['Problem.Name'].unique():\n",
    "        problem_data = student_data[student_data['Problem.Name'] == problem].sort_values('time_step')\n",
    "        \n",
    "        # Look for hint request followed by attempt\n",
    "        for i in range(len(problem_data) - 1):\n",
    "            current_action = str(problem_data.iloc[i]['Action']).lower()\n",
    "            next_outcome = str(problem_data.iloc[i + 1]['Outcome']).upper()\n",
    "            \n",
    "            if 'hint' in current_action and next_outcome in ['OK', 'CORRECT', 'WRONG', 'ERROR']:\n",
    "                hint_pattern_count += 1\n",
    "                break\n",
    "    \n",
    "    features['hint_usage_pattern'] = hint_pattern_count / total_problems if total_problems > 0 else 0\n",
    "    \n",
    "    # 3. Optimal Help Seeking Behavior (time spent + accuracy + hint usage)\n",
    "    # Time spent on problem\n",
    "    problem_times = []\n",
    "    problem_accuracy = []\n",
    "    problem_hint_usage = []\n",
    "    \n",
    "    for problem in student_data['Problem.Name'].unique():\n",
    "        problem_data = student_data[student_data['Problem.Name'] == problem].sort_values('time_step')\n",
    "        \n",
    "        if len(problem_data) > 1:\n",
    "            time_spent = problem_data['time_step'].max() - problem_data['time_step'].min()\n",
    "            problem_times.append(time_spent)\n",
    "        \n",
    "        # Accuracy for this problem\n",
    "        correct_attempts = (problem_data['Outcome'].isin(['OK', 'CORRECT'])).sum()\n",
    "        total_attempts = len(problem_data)\n",
    "        accuracy = correct_attempts / total_attempts if total_attempts > 0 else 0\n",
    "        problem_accuracy.append(accuracy)\n",
    "        \n",
    "        # Hint usage for this problem\n",
    "        hint_requests = problem_data['Action'].str.contains('hint', case=False, na=False).sum()\n",
    "        problem_hint_usage.append(hint_requests)\n",
    "    \n",
    "    features['avg_time_per_problem'] = np.mean(problem_times) if problem_times else 0\n",
    "    features['avg_problem_accuracy'] = np.mean(problem_accuracy) if problem_accuracy else 0\n",
    "    features['avg_hints_per_problem'] = np.mean(problem_hint_usage) if problem_hint_usage else 0\n",
    "    \n",
    "    # Combined optimal help seeking score\n",
    "    # Higher time + higher accuracy + moderate hint usage = better metacognition\n",
    "    time_score = min(features['avg_time_per_problem'] / 60, 1)  # Normalize to max 1 minute\n",
    "    accuracy_val = features['avg_problem_accuracy']\n",
    "    hint_score = 1 - min(features['avg_hints_per_problem'] / 3, 1)  # Penalize excessive hints\n",
    "    \n",
    "    features['optimal_help_seeking'] = (time_score + accuracy_val + hint_score) / 3\n",
    "    \n",
    "    # 4. Length of time spent on Concept Building\n",
    "    # Approximate concept building as time spent on problems with multiple attempts\n",
    "    concept_building_time = 0\n",
    "    for problem in student_data['Problem.Name'].unique():\n",
    "        problem_data = student_data[student_data['Problem.Name'] == problem].sort_values('time_step')\n",
    "        \n",
    "        if len(problem_data) > 2:  # Multiple attempts suggest concept building\n",
    "            time_spent = problem_data['time_step'].max() - problem_data['time_step'].min()\n",
    "            concept_building_time += time_spent\n",
    "    \n",
    "    features['concept_building_time'] = concept_building_time\n",
    "    \n",
    "    # Additional metacognition-related features\n",
    "    # 5. Self-regulation indicators\n",
    "    features['total_problems_attempted'] = total_problems\n",
    "    features['avg_attempts_per_problem'] = len(student_data) / total_problems if total_problems > 0 else 0\n",
    "    features['error_recovery_rate'] = 0\n",
    "    \n",
    "    # Error recovery: correct answer after error\n",
    "    for problem in student_data['Problem.Name'].unique():\n",
    "        problem_data = student_data[student_data['Problem.Name'] == problem].sort_values('time_step')\n",
    "        \n",
    "        for i in range(len(problem_data) - 1):\n",
    "            if problem_data.iloc[i]['Outcome'] in ['ERROR', 'WRONG']:\n",
    "                if problem_data.iloc[i + 1]['Outcome'] in ['OK', 'CORRECT']:\n",
    "                    features['error_recovery_rate'] += 1\n",
    "                    break\n",
    "    \n",
    "    features['error_recovery_rate'] = features['error_recovery_rate'] / total_problems if total_problems > 0 else 0\n",
    "    \n",
    "    # 6. Persistence indicators\n",
    "    features['session_persistence'] = student_data['Session.Id'].nunique()  # Number of different sessions\n",
    "    features['total_interaction_time'] = student_data['time_step'].max() - student_data['time_step'].min() if len(student_data) > 1 else 0\n",
    "    \n",
    "    # 7. Strategic help-seeking (help level progression)\n",
    "    if 'Help.Level' in student_data.columns:\n",
    "        help_progression = []\n",
    "        for problem in student_data['Problem.Name'].unique():\n",
    "            problem_data = student_data[student_data['Problem.Name'] == problem].sort_values('time_step')\n",
    "            help_levels = problem_data['Help.Level'].dropna()\n",
    "            \n",
    "            if len(help_levels) > 1:\n",
    "                # Check if help levels increase (strategic escalation)\n",
    "                increasing = (help_levels.diff() > 0).sum()\n",
    "                help_progression.append(increasing / len(help_levels))\n",
    "        \n",
    "        features['strategic_help_progression'] = np.mean(help_progression) if help_progression else 0\n",
    "    else:\n",
    "        features['strategic_help_progression'] = 0\n",
    "    \n",
    "    features_list.append(features)\n",
    "\n",
    "# Create features dataframe\n",
    "features_df = pd.DataFrame(features_list)\n",
    "print(f\"\\nMetacognition features engineered for {len(features_df)} students\")\n",
    "print(f\"Total features: {len(features_df.columns) - 1}\")\n",
    "\n",
    "print(\"\\nMETACOGNITION-FOCUSED FEATURES CREATED:\")\n",
    "print(\"-\" * 50)\n",
    "metacog_features = [\n",
    "    'avg_help_level', 'hint_usage_pattern', 'optimal_help_seeking', \n",
    "    'concept_building_time', 'avg_time_per_problem', 'avg_problem_accuracy',\n",
    "    'error_recovery_rate', 'strategic_help_progression'\n",
    "]\n",
    "\n",
    "for feat in metacog_features:\n",
    "    if feat in features_df.columns:\n",
    "        mean_val = features_df[feat].mean()\n",
    "        print(f\"  ✓ {feat}: mean = {mean_val:.3f}\")\n",
    "\n",
    "# Display feature statistics\n",
    "print(f\"\\nFeature summary:\")\n",
    "print(features_df.describe().round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e98fa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# DATA MERGING AND PREPARATION\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\nMERGING FEATURES WITH TARGET VARIABLE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Merge features with metacognition scores (including Post1Post2_Avg as a feature)\n",
    "df = features_df.merge(metacognition_data[['Anon.Student.Id', 'metacognition_score', 'metacognition_category', 'Post1Post2_Avg']], \n",
    "                       on='Anon.Student.Id', how='inner')\n",
    "\n",
    "print(f\"Final dataset: {len(df)} students\")\n",
    "print(f\"Features: {len(df.columns) - 4}\")  # Excluding ID, target, and Post1Post2_Avg columns\n",
    "\n",
    "# Handle missing values\n",
    "print(f\"\\nMissing values per column:\")\n",
    "missing_counts = df.isnull().sum()\n",
    "for col, count in missing_counts.items():\n",
    "    if count > 0:\n",
    "        print(f\"  {col}: {count}\")\n",
    "\n",
    "# Fill missing values with median for numerical columns (excluding target and Post1Post2_Avg)\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "numerical_cols = [col for col in numerical_cols if col not in ['metacognition_score', 'Post1Post2_Avg']]\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "df[numerical_cols] = imputer.fit_transform(df[numerical_cols])\n",
    "\n",
    "print(f\"\\nData preparation complete!\")\n",
    "print(f\"Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6791d96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# QWK METRIC DEFINITION\n",
    "# ==============================================================================\n",
    "\n",
    "def qwk(y_true, y_pred):\n",
    "    \"\"\"Quadratic Weighted Kappa - primary metric for ordinal classification\"\"\"\n",
    "    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
    "\n",
    "def qwk_scorer(estimator, X, y):\n",
    "    \"\"\"Custom scorer for cross-validation\"\"\"\n",
    "    return qwk(y, estimator.predict(X))\n",
    "\n",
    "print(\"QWK metric defined for ordinal classification evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e71c605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# LOAD TRAINED MODEL AND MAKE PREDICTIONS\n",
    "# ==============================================================================\n",
    "print(\"Loading trained meta model...\")\n",
    "\n",
    "# Load the saved model, label encoder, and feature columns\n",
    "with open('meta_model.pkl', 'rb') as f:\n",
    "    model, le, feature_cols = pickle.load(f)\n",
    "\n",
    "print(f\"✓ Model loaded successfully!\")\n",
    "print(f\"✓ Model type: {type(model).__name__}\")\n",
    "print(f\"✓ Number of features: {len(feature_cols)}\")\n",
    "print(f\"✓ Label encoder loaded: {type(le).__name__}\")\n",
    "\n",
    "# Prepare features for prediction\n",
    "print(\"\\nPreparing features for prediction...\")\n",
    "\n",
    "# Select only the features used during training\n",
    "X_test = df[feature_cols].copy()\n",
    "\n",
    "# Handle missing values (same as training)\n",
    "print(\"Handling missing values...\")\n",
    "X_test = X_test.replace([np.inf, -np.inf], np.nan)\n",
    "X_test = X_test.fillna(0)  # Fill with 0 for missing values\n",
    "\n",
    "print(f\"✓ Test features shape: {X_test.shape}\")\n",
    "print(f\"✓ Features with missing values: {X_test.isnull().sum().sum()}\")\n",
    "\n",
    "# Make predictions\n",
    "print(\"\\nMaking predictions...\")\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Convert predictions back to original scale using label encoder\n",
    "if hasattr(le, 'inverse_transform'):\n",
    "    predictions_original = le.inverse_transform(predictions)\n",
    "else:\n",
    "    predictions_original = predictions\n",
    "\n",
    "print(f\"✓ Predictions made for {len(predictions)} students\")\n",
    "print(f\"✓ Prediction range: {predictions_original.min():.2f} to {predictions_original.max():.2f}\")\n",
    "\n",
    "# Create results dataframe\n",
    "results_df = pd.DataFrame({\n",
    "    'Anon.Student.Id': df['Anon.Student.Id'],\n",
    "    'Post3Meta_Predicted': predictions_original\n",
    "})\n",
    "\n",
    "print(f\"\\nResults shape: {results_df.shape}\")\n",
    "print(\"\\nFirst 10 predictions:\")\n",
    "print(results_df.head(10))\n",
    "\n",
    "# Save predictions\n",
    "output_file = 'meta_predictions.csv'\n",
    "results_df.to_csv(output_file, index=False)\n",
    "print(f\"\\n✓ Predictions saved to '{output_file}'\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
