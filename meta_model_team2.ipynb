{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c840628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METACOGNITION PREDICTION MODEL\n",
      "============================================================\n",
      "Predicting Post3Meta scores using behavioral features\n",
      "Target: Post3Meta (Metacognition Assessment)\n",
      "Features: Learning behavior patterns and help-seeking\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning imports - CLASSIFICATION ONLY\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, cohen_kappa_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"METACOGNITION PREDICTION MODEL\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Predicting Post3Meta scores using behavioral features\")\n",
    "print(\"Target: Post3Meta (Metacognition Assessment)\")\n",
    "print(\"Features: Learning behavior patterns and help-seeking\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87827dd",
   "metadata": {},
   "source": [
    "# Training Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a369467",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main = pd.read_csv(\"./raw_data/training_set_with_formatted_time.csv\")\n",
    "df_ws = pd.read_csv(\"./raw_data/workspace_summary_train.csv\")\n",
    "df_scores = pd.read_csv(\"./raw_data/student_scores_train.csv\")\n",
    "\n",
    "df_main.drop_duplicates(inplace=True)\n",
    "df_ws.drop_duplicates(inplace=True)\n",
    "df_scores.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa5802e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary columns\n",
    "df_main.drop(columns=['CF..Anon.School.Id.', 'CF..Anon.Class.Id.', 'Time', 'formatted_time'], inplace=True)\n",
    "\n",
    "# Remove rows containing 'OK_AMBIGUOUS'\n",
    "df_main = df_main[df_main['Outcome'] != 'OK_AMBIGUOUS']\n",
    "\n",
    "df_main.sort_values(by=['Anon.Student.Id', 'datetime'], inplace=True)\n",
    "\n",
    "df_main['datetime'] = pd.to_datetime(\n",
    "    df_main['datetime'],\n",
    "    infer_datetime_format=True,\n",
    "    errors='coerce'       # turns invalid parses into NaT\n",
    ")\n",
    "\n",
    "# Generate time steps\n",
    "df_main['time_step'] = df_main.groupby('Anon.Student.Id')['datetime'].rank(method='first') - 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcdad92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main.to_csv('preprocessed_data/df_main_allws.csv', index=False)\n",
    "df_ws.to_csv('preprocessed_data/df_ws_allws.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c0f8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace_ids_to_remove = [\n",
    "    'worksheet_grapher_a1_lin_mod_mult_rep',\n",
    "    'equation_line_2',\n",
    "    'analyzing_models_2step_rationals',\n",
    "    'multiple_representations_of_linear_functions',\n",
    "    'worksheet_grapher_a1_slope_intercept_integer',\n",
    "    'worksheet_grapher_a1_slope_intercept_decimal',\n",
    "    'connecting_slope_intercept_and_point_slope_forms',\n",
    "    'equation_line_1',\n",
    "    'equation_line_3',\n",
    "    'worksheet_grapher_a1_mod_initial_plus_point',\n",
    "    'worksheet_grapher_a1_mod_two_points',\n",
    "    'modeling_linear_equations_in_standard_form',\n",
    "    'graph_setup_linear_equation-1',\n",
    "    'graph_setup_linear_equation-2',\n",
    "    'classifying_relations_and_functions',\n",
    "    'introduction_to_functions',\n",
    "    'graphs_of_functions',\n",
    "    'graphs_of_functions-1',\n",
    "    'compare_functions_diff_reps_linear_relationships'\n",
    "]\n",
    "\n",
    "\n",
    "df_main = df_main[~df_main['Level..Workspace.Id.'].isin(workspace_ids_to_remove)]\n",
    "df_ws = df_ws[~df_ws['workspace'].isin(workspace_ids_to_remove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5fbcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 4 dataframes, each with cleaned pre/post scores and imputed NaNs with median\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "df_cleaned_math = df_scores[[\"Anon.Student.Id\", \"PreMath\", \"PostMath\"]].copy()\n",
    "df_cleaned_math = df_cleaned_math.dropna(subset=[\"PostMath\"])\n",
    "\n",
    "df_cleaned_map = df_scores[[\"Anon.Student.Id\", \"PreMAP\", \"Post1MAP\", \"Post2MAP\", \"Post3MAP\"]].copy()\n",
    "df_cleaned_map[[\"PreMAP\", \"Post1MAP\", \"Post2MAP\", \"Post3MAP\"]] = imputer.fit_transform(df_cleaned_map[[\"PreMAP\", \"Post1MAP\", \"Post2MAP\", \"Post3MAP\"]])\n",
    "\n",
    "df_cleaned_se = df_scores[[\"Anon.Student.Id\", \"PreSE\", \"Post1SE\", \"Post2SE\", \"Post3SE\"]].copy()\n",
    "df_cleaned_se[[\"PreSE\", \"Post1SE\", \"Post2SE\", \"Post3SE\"]] = imputer.fit_transform(df_cleaned_se[[\"PreSE\", \"Post1SE\", \"Post2SE\", \"Post3SE\"]])\n",
    "\n",
    "df_cleaned_meta = df_scores[[\"Anon.Student.Id\", \"PreMeta\", \"Post1Meta\", \"Post2Meta\", \"Post3Meta\"]].copy()\n",
    "df_cleaned_meta[[\"PreMeta\", \"Post1Meta\", \"Post2Meta\", \"Post3Meta\"]] = imputer.fit_transform(df_cleaned_meta[[\"PreMeta\", \"Post1Meta\", \"Post2Meta\", \"Post3Meta\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef8392c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all to csv and store in processed_data folder\n",
    "df_main.to_csv(\"preprocessed_data/df_main.csv\", index=False)\n",
    "df_ws.to_csv(\"preprocessed_data/df_ws.csv\", index=False)\n",
    "df_scores.to_csv(\"preprocessed_data/df_scores.csv\", index=False)\n",
    "df_cleaned_meta.to_csv(\"preprocessed_data/df_cleaned_meta.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5090d55f",
   "metadata": {},
   "source": [
    "# Loading Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20f4a82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LOADING AND VALIDATING DATA\n",
      "==================================================\n",
      "✓ Main interaction data: 856,606 records\n",
      "✓ Workspace data: 12,635 workspace sessions\n",
      "✓ Metacognition assessments: 539 students\n",
      "\n",
      "VALIDATING METACOGNITION DATA:\n",
      "----------------------------------------\n",
      "Meta columns: ['Anon.Student.Id', 'PreMeta', 'Post1Meta', 'Post2Meta', 'Post3Meta']\n",
      "✓ Post3Meta found: 539 non-null values\n",
      "  Range: 1.00 to 7.00\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# DATA LOADING AND VALIDATION\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\nLOADING AND VALIDATING DATA\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Load all data files\n",
    "df_main = pd.read_csv('preprocessed_data/df_main_allws.csv')\n",
    "df_ws = pd.read_csv('preprocessed_data/df_ws_allws.csv')\n",
    "df_cleaned_meta = pd.read_csv('preprocessed_data/df_cleaned_meta.csv')\n",
    "\n",
    "print(f\"✓ Main interaction data: {len(df_main):,} records\")\n",
    "print(f\"✓ Workspace data: {len(df_ws):,} workspace sessions\")\n",
    "print(f\"✓ Metacognition assessments: {len(df_cleaned_meta):,} students\")\n",
    "\n",
    "print(f\"\\nVALIDATING METACOGNITION DATA:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"Meta columns:\", df_cleaned_meta.columns.tolist())\n",
    "\n",
    "# Check for Post3Meta specifically\n",
    "if 'Post3Meta' not in df_cleaned_meta.columns:\n",
    "    print(\"ERROR: Post3Meta column not found!\")\n",
    "    print(\"Available Meta columns:\", [col for col in df_cleaned_meta.columns if 'Meta' in col])\n",
    "else:\n",
    "    print(f\"✓ Post3Meta found: {df_cleaned_meta['Post3Meta'].notna().sum()} non-null values\")\n",
    "    print(f\"  Range: {df_cleaned_meta['Post3Meta'].min():.2f} to {df_cleaned_meta['Post3Meta'].max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "151357a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CREATING TARGET VARIABLE\n",
      "==================================================\n",
      "Students with Post3Meta scores: 539\n",
      "\n",
      "Metacognition score statistics:\n",
      "  Mean: 4.28\n",
      "  Std: 0.98\n",
      "  Range: 1.00 - 7.00\n",
      "\n",
      "Metacognition category distribution:\n",
      "  • Medium: 330 students (61.2%)\n",
      "  • Low: 151 students (28.0%)\n",
      "  • High: 58 students (10.8%)\n",
      "\n",
      "Target variable created successfully!\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# TARGET VARIABLE CREATION\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\nCREATING TARGET VARIABLE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Filter students with Post3Meta scores\n",
    "metacognition_data = df_cleaned_meta[df_cleaned_meta['PreMeta'].notna() & df_cleaned_meta['Post3Meta'].notna()].copy()\n",
    "print(f\"Students with Post3Meta scores: {len(metacognition_data)}\")\n",
    "\n",
    "# Create both continuous and categorical targets\n",
    "metacognition_data['metacognition_score'] = metacognition_data['Post3Meta']\n",
    "\n",
    "# Create categorical version (Low, Medium, High)\n",
    "def categorize_metacognition(score):\n",
    "    if score <= 4.0:\n",
    "        return 'Low'\n",
    "    elif score <= 5.5:\n",
    "        return 'Medium'\n",
    "    else:\n",
    "        return 'High'\n",
    "\n",
    "metacognition_data['metacognition_category'] = metacognition_data['metacognition_score'].apply(categorize_metacognition)\n",
    "\n",
    "# Show distribution\n",
    "print(f\"\\nMetacognition score statistics:\")\n",
    "print(f\"  Mean: {metacognition_data['metacognition_score'].mean():.2f}\")\n",
    "print(f\"  Std: {metacognition_data['metacognition_score'].std():.2f}\")\n",
    "print(f\"  Range: {metacognition_data['metacognition_score'].min():.2f} - {metacognition_data['metacognition_score'].max():.2f}\")\n",
    "\n",
    "category_counts = metacognition_data['metacognition_category'].value_counts()\n",
    "print(f\"\\nMetacognition category distribution:\")\n",
    "for category, count in category_counts.items():\n",
    "    percentage = (count / len(metacognition_data)) * 100\n",
    "    print(f\"  • {category}: {count} students ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTarget variable created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51758a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ENGINEERING METACOGNITION-FOCUSED FEATURES\n",
      "==================================================\n",
      "Filtering data for 539 students with metacognition scores\n",
      "Behavioral data: 855,024 interactions\n",
      "\n",
      "Metacognition features engineered for 539 students\n",
      "Total features: 13\n",
      "\n",
      "METACOGNITION-FOCUSED FEATURES CREATED:\n",
      "--------------------------------------------------\n",
      "  ✓ avg_help_level: mean = 2.056\n",
      "  ✓ hint_usage_pattern: mean = 0.451\n",
      "  ✓ optimal_help_seeking: mean = 0.471\n",
      "  ✓ concept_building_time: mean = 2153.840\n",
      "  ✓ avg_time_per_problem: mean = 31.459\n",
      "  ✓ avg_problem_accuracy: mean = 0.710\n",
      "  ✓ error_recovery_rate: mean = 0.512\n",
      "  ✓ strategic_help_progression: mean = 0.108\n",
      "\n",
      "Feature summary:\n",
      "       avg_help_level  hint_usage_pattern  avg_time_per_problem  \\\n",
      "count         539.000             539.000               539.000   \n",
      "mean            2.056               0.451                31.459   \n",
      "std             0.200               0.177                21.851   \n",
      "min             0.000               0.000                10.500   \n",
      "25%             1.981               0.330                22.595   \n",
      "50%             2.034               0.429                29.130   \n",
      "75%             2.144               0.557                36.029   \n",
      "max             2.500               1.000               394.218   \n",
      "\n",
      "       avg_problem_accuracy  avg_hints_per_problem  optimal_help_seeking  \\\n",
      "count               539.000                539.000               539.000   \n",
      "mean                  0.710                  4.147                 0.471   \n",
      "std                   0.117                  3.321                 0.112   \n",
      "min                   0.323                  0.000                 0.258   \n",
      "25%                   0.660                  1.805                 0.390   \n",
      "50%                   0.738                  3.412                 0.457   \n",
      "75%                   0.785                  5.404                 0.542   \n",
      "max                   0.958                 19.250                 0.908   \n",
      "\n",
      "       concept_building_time  total_problems_attempted  \\\n",
      "count                539.000                   539.000   \n",
      "mean                2153.840                    60.640   \n",
      "std                 2686.263                    35.522   \n",
      "min                   42.000                     2.000   \n",
      "25%                  664.000                    26.000   \n",
      "50%                 1820.000                    68.000   \n",
      "75%                 2888.000                    83.000   \n",
      "max                46912.000                   211.000   \n",
      "\n",
      "       avg_attempts_per_problem  error_recovery_rate  session_persistence  \\\n",
      "count                   539.000              539.000              539.000   \n",
      "mean                     25.320                0.512               14.063   \n",
      "std                       5.335                0.144                9.907   \n",
      "min                      11.500                0.200                1.000   \n",
      "25%                      21.934                0.422                6.000   \n",
      "50%                      25.000                0.493               13.000   \n",
      "75%                      28.063                0.582               19.000   \n",
      "max                      53.222                1.000               62.000   \n",
      "\n",
      "       total_interaction_time  strategic_help_progression  \n",
      "count                 539.000                     539.000  \n",
      "mean                 1583.740                       0.108  \n",
      "std                  1087.884                       0.078  \n",
      "min                    45.000                       0.000  \n",
      "25%                   650.000                       0.056  \n",
      "50%                  1578.000                       0.091  \n",
      "75%                  2149.000                       0.135  \n",
      "max                  7792.000                       0.490  \n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# FEATURE ENGINEERING - METACOGNITION-FOCUSED FEATURES\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\nENGINEERING METACOGNITION-FOCUSED FEATURES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get students with metacognition scores\n",
    "target_students = metacognition_data['Anon.Student.Id'].unique()\n",
    "df_filtered = df_main[df_main['Anon.Student.Id'].isin(target_students)].copy()\n",
    "\n",
    "print(f\"Filtering data for {len(target_students)} students with metacognition scores\")\n",
    "print(f\"Behavioral data: {len(df_filtered):,} interactions\")\n",
    "\n",
    "# Feature engineering by student\n",
    "features_list = []\n",
    "\n",
    "for student_id in target_students:\n",
    "    student_data = df_filtered[df_filtered['Anon.Student.Id'] == student_id].copy()\n",
    "    \n",
    "    if len(student_data) == 0:\n",
    "        continue\n",
    "    \n",
    "    features = {'Anon.Student.Id': student_id}\n",
    "    \n",
    "    # 1. Average Help Level (exclude 0 if 0 = no hint)\n",
    "    if 'Help.Level' in student_data.columns:\n",
    "        help_levels = student_data['Help.Level'].dropna()\n",
    "        help_levels_nonzero = help_levels[help_levels > 0]  # Exclude 0 if 0 = no hint\n",
    "        features['avg_help_level'] = help_levels_nonzero.mean() if len(help_levels_nonzero) > 0 else 0\n",
    "    else:\n",
    "        features['avg_help_level'] = 0\n",
    "    \n",
    "    # 2. Hint Usage Pattern (% of problems with Hint Requested followed by OK/WRONG)\n",
    "    hint_pattern_count = 0\n",
    "    total_problems = student_data['Problem.Name'].nunique()\n",
    "    \n",
    "    for problem in student_data['Problem.Name'].unique():\n",
    "        problem_data = student_data[student_data['Problem.Name'] == problem].sort_values('time_step')\n",
    "        \n",
    "        # Look for hint request followed by attempt\n",
    "        for i in range(len(problem_data) - 1):\n",
    "            current_action = str(problem_data.iloc[i]['Action']).lower()\n",
    "            next_outcome = str(problem_data.iloc[i + 1]['Outcome']).upper()\n",
    "            \n",
    "            if 'hint' in current_action and next_outcome in ['OK', 'CORRECT', 'WRONG', 'ERROR']:\n",
    "                hint_pattern_count += 1\n",
    "                break\n",
    "    \n",
    "    features['hint_usage_pattern'] = hint_pattern_count / total_problems if total_problems > 0 else 0\n",
    "    \n",
    "    # 3. Optimal Help Seeking Behavior (time spent + accuracy + hint usage)\n",
    "    # Time spent on problem\n",
    "    problem_times = []\n",
    "    problem_accuracy = []\n",
    "    problem_hint_usage = []\n",
    "    \n",
    "    for problem in student_data['Problem.Name'].unique():\n",
    "        problem_data = student_data[student_data['Problem.Name'] == problem].sort_values('time_step')\n",
    "        \n",
    "        if len(problem_data) > 1:\n",
    "            time_spent = problem_data['time_step'].max() - problem_data['time_step'].min()\n",
    "            problem_times.append(time_spent)\n",
    "        \n",
    "        # Accuracy for this problem\n",
    "        correct_attempts = (problem_data['Outcome'].isin(['OK', 'CORRECT'])).sum()\n",
    "        total_attempts = len(problem_data)\n",
    "        accuracy = correct_attempts / total_attempts if total_attempts > 0 else 0\n",
    "        problem_accuracy.append(accuracy)\n",
    "        \n",
    "        # Hint usage for this problem\n",
    "        hint_requests = problem_data['Action'].str.contains('hint', case=False, na=False).sum()\n",
    "        problem_hint_usage.append(hint_requests)\n",
    "    \n",
    "    features['avg_time_per_problem'] = np.mean(problem_times) if problem_times else 0\n",
    "    features['avg_problem_accuracy'] = np.mean(problem_accuracy) if problem_accuracy else 0\n",
    "    features['avg_hints_per_problem'] = np.mean(problem_hint_usage) if problem_hint_usage else 0\n",
    "    \n",
    "    # Combined optimal help seeking score\n",
    "    # Higher time + higher accuracy + moderate hint usage = better metacognition\n",
    "    time_score = min(features['avg_time_per_problem'] / 60, 1)  # Normalize to max 1 minute\n",
    "    accuracy_val = features['avg_problem_accuracy']\n",
    "    hint_score = 1 - min(features['avg_hints_per_problem'] / 3, 1)  # Penalize excessive hints\n",
    "    \n",
    "    features['optimal_help_seeking'] = (time_score + accuracy_val + hint_score) / 3\n",
    "    \n",
    "    # 4. Length of time spent on Concept Building\n",
    "    # Approximate concept building as time spent on problems with multiple attempts\n",
    "    concept_building_time = 0\n",
    "    for problem in student_data['Problem.Name'].unique():\n",
    "        problem_data = student_data[student_data['Problem.Name'] == problem].sort_values('time_step')\n",
    "        \n",
    "        if len(problem_data) > 2:  # Multiple attempts suggest concept building\n",
    "            time_spent = problem_data['time_step'].max() - problem_data['time_step'].min()\n",
    "            concept_building_time += time_spent\n",
    "    \n",
    "    features['concept_building_time'] = concept_building_time\n",
    "    \n",
    "    # Additional metacognition-related features\n",
    "    # 5. Self-regulation indicators\n",
    "    features['total_problems_attempted'] = total_problems\n",
    "    features['avg_attempts_per_problem'] = len(student_data) / total_problems if total_problems > 0 else 0\n",
    "    features['error_recovery_rate'] = 0\n",
    "    \n",
    "    # Error recovery: correct answer after error\n",
    "    for problem in student_data['Problem.Name'].unique():\n",
    "        problem_data = student_data[student_data['Problem.Name'] == problem].sort_values('time_step')\n",
    "        \n",
    "        for i in range(len(problem_data) - 1):\n",
    "            if problem_data.iloc[i]['Outcome'] in ['ERROR', 'WRONG']:\n",
    "                if problem_data.iloc[i + 1]['Outcome'] in ['OK', 'CORRECT']:\n",
    "                    features['error_recovery_rate'] += 1\n",
    "                    break\n",
    "    \n",
    "    features['error_recovery_rate'] = features['error_recovery_rate'] / total_problems if total_problems > 0 else 0\n",
    "    \n",
    "    # 6. Persistence indicators\n",
    "    features['session_persistence'] = student_data['Session.Id'].nunique()  # Number of different sessions\n",
    "    features['total_interaction_time'] = student_data['time_step'].max() - student_data['time_step'].min() if len(student_data) > 1 else 0\n",
    "    \n",
    "    # 7. Strategic help-seeking (help level progression)\n",
    "    if 'Help.Level' in student_data.columns:\n",
    "        help_progression = []\n",
    "        for problem in student_data['Problem.Name'].unique():\n",
    "            problem_data = student_data[student_data['Problem.Name'] == problem].sort_values('time_step')\n",
    "            help_levels = problem_data['Help.Level'].dropna()\n",
    "            \n",
    "            if len(help_levels) > 1:\n",
    "                # Check if help levels increase (strategic escalation)\n",
    "                increasing = (help_levels.diff() > 0).sum()\n",
    "                help_progression.append(increasing / len(help_levels))\n",
    "        \n",
    "        features['strategic_help_progression'] = np.mean(help_progression) if help_progression else 0\n",
    "    else:\n",
    "        features['strategic_help_progression'] = 0\n",
    "    \n",
    "    features_list.append(features)\n",
    "\n",
    "# Create features dataframe\n",
    "features_df = pd.DataFrame(features_list)\n",
    "print(f\"\\nMetacognition features engineered for {len(features_df)} students\")\n",
    "print(f\"Total features: {len(features_df.columns) - 1}\")\n",
    "\n",
    "print(\"\\nMETACOGNITION-FOCUSED FEATURES CREATED:\")\n",
    "print(\"-\" * 50)\n",
    "metacog_features = [\n",
    "    'avg_help_level', 'hint_usage_pattern', 'optimal_help_seeking', \n",
    "    'concept_building_time', 'avg_time_per_problem', 'avg_problem_accuracy',\n",
    "    'error_recovery_rate', 'strategic_help_progression'\n",
    "]\n",
    "\n",
    "for feat in metacog_features:\n",
    "    if feat in features_df.columns:\n",
    "        mean_val = features_df[feat].mean()\n",
    "        print(f\"  ✓ {feat}: mean = {mean_val:.3f}\")\n",
    "\n",
    "# Display feature statistics\n",
    "print(f\"\\nFeature summary:\")\n",
    "print(features_df.describe().round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4149e15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MERGING FEATURES WITH TARGET VARIABLE\n",
      "==================================================\n",
      "Final dataset: 539 students\n",
      "Features: 13\n",
      "\n",
      "Missing values per column:\n",
      "\n",
      "Data preparation complete!\n",
      "Shape: (539, 17)\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# DATA MERGING AND PREPARATION\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\nMERGING FEATURES WITH TARGET VARIABLE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Merge features with metacognition scores (including Post3Meta as a feature)\n",
    "df = features_df.merge(metacognition_data[['Anon.Student.Id', 'metacognition_score', 'metacognition_category', 'Post3Meta']], \n",
    "                       on='Anon.Student.Id', how='inner')\n",
    "\n",
    "print(f\"Final dataset: {len(df)} students\")\n",
    "print(f\"Features: {len(df.columns) - 4}\")  # Excluding ID, target, and Post3Meta columns\n",
    "\n",
    "# Handle missing values\n",
    "print(f\"\\nMissing values per column:\")\n",
    "missing_counts = df.isnull().sum()\n",
    "for col, count in missing_counts.items():\n",
    "    if count > 0:\n",
    "        print(f\"  {col}: {count}\")\n",
    "\n",
    "# Fill missing values with median for numerical columns (excluding target and Post3Meta)\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "numerical_cols = [col for col in numerical_cols if col not in ['metacognition_score', 'Post3Meta']]\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "df[numerical_cols] = imputer.fit_transform(df[numerical_cols])\n",
    "\n",
    "print(f\"\\nData preparation complete!\")\n",
    "print(f\"Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02b7ca77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QWK metric defined for ordinal classification evaluation\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# QWK METRIC DEFINITION\n",
    "# ==============================================================================\n",
    "\n",
    "def qwk(y_true, y_pred):\n",
    "    \"\"\"Quadratic Weighted Kappa - primary metric for ordinal classification\"\"\"\n",
    "    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
    "\n",
    "def qwk_scorer(estimator, X, y):\n",
    "    \"\"\"Custom scorer for cross-validation\"\"\"\n",
    "    return qwk(y, estimator.predict(X))\n",
    "\n",
    "print(\"QWK metric defined for ordinal classification evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d655958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 431 samples\n",
      "Test set: 108 samples\n",
      "Features: 14\n",
      "\n",
      "Train distribution:\n",
      "metacognition_score\n",
      "1      4\n",
      "2     29\n",
      "3     44\n",
      "4    230\n",
      "5     78\n",
      "6     42\n",
      "7      4\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test distribution:\n",
      "metacognition_score\n",
      "1     1\n",
      "2     7\n",
      "3    11\n",
      "4    57\n",
      "5    20\n",
      "6    11\n",
      "7     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Feature names:\n",
      "   1. avg_help_level\n",
      "   2. hint_usage_pattern\n",
      "   3. avg_time_per_problem\n",
      "   4. avg_problem_accuracy\n",
      "   5. avg_hints_per_problem\n",
      "   6. optimal_help_seeking\n",
      "   7. concept_building_time\n",
      "   8. total_problems_attempted\n",
      "   9. avg_attempts_per_problem\n",
      "  10. error_recovery_rate\n",
      "  11. session_persistence\n",
      "  12. total_interaction_time\n",
      "  13. strategic_help_progression\n",
      "  14. Post3Meta\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# TRAIN/TEST SPLIT - STRATIFIED FOR CLASSIFICATION\n",
    "# ==============================================================================\n",
    "\n",
    "# Define features and target\n",
    "feature_cols = [col for col in df.columns if col not in [\n",
    "    'Anon.Student.Id', 'metacognition_score', 'metacognition_category'\n",
    "]]\n",
    "X = df[feature_cols]\n",
    "y = df['metacognition_score'].round().astype(int)  # Round to nearest integer for classification\n",
    "\n",
    "# Stratified split to preserve class distribution\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")\n",
    "print(f\"Features: {len(feature_cols)}\")\n",
    "\n",
    "print(\"\\nTrain distribution:\")\n",
    "print(y_train.value_counts().sort_index())\n",
    "print(\"\\nTest distribution:\")\n",
    "print(y_test.value_counts().sort_index())\n",
    "\n",
    "print(\"\\nFeature names:\")\n",
    "for i, feat in enumerate(feature_cols, 1):\n",
    "    print(f\"  {i:2d}. {feat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb3b0f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification models defined:\n",
      "  ✓ RandomForest\n",
      "  ✓ GradientBoosting\n",
      "  ✓ XGBoost\n"
     ]
    }
   ],
   "source": [
    "# Define classification models optimized for ordinal data\n",
    "models = {\n",
    "    'RandomForest': RandomForestClassifier(\n",
    "        n_estimators=100, \n",
    "        random_state=42, \n",
    "        class_weight='balanced'\n",
    "    ),\n",
    "    'GradientBoosting': GradientBoostingClassifier(\n",
    "        n_estimators=100,\n",
    "        random_state=42\n",
    "    ),\n",
    "    'XGBoost': XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        random_state=42,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='mlogloss'\n",
    "    )\n",
    "}\n",
    "\n",
    "print(\"Classification models defined:\")\n",
    "for name in models.keys():\n",
    "    print(f\"  ✓ {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b1d79b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation with QWK metric:\n",
      "----------------------------------------\n",
      "RandomForest        : QWK = 0.878 ± 0.043\n",
      "GradientBoosting    : QWK = 1.000 ± 0.000\n",
      "XGBoost             : QWK = 0.999 ± 0.002\n",
      "\n",
      "Best CV model: GradientBoosting (QWK: 1.000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:01:08] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:01:08] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:01:08] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:01:08] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:01:08] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CROSS-VALIDATION WITH QWK METRIC (FIXED FOR XGBOOST)\n",
    "# ==============================================================================\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_results = {}\n",
    "\n",
    "print(\"Cross-validation with QWK metric:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Create label encoder for XGBoost compatibility\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "\n",
    "for name, model in models.items():\n",
    "    if name == 'XGBoost':\n",
    "        # Use encoded labels for XGBoost (0-6 instead of 1-7)\n",
    "        scores = cross_val_score(model, X_train, y_train_encoded, cv=cv, scoring=qwk_scorer, n_jobs=-1)\n",
    "    else:\n",
    "        # Use original labels for other models\n",
    "        scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=qwk_scorer, n_jobs=-1)\n",
    "    \n",
    "    cv_results[name] = {\n",
    "        'mean_qwk': np.mean(scores),\n",
    "        'std_qwk': np.std(scores),\n",
    "        'scores': scores\n",
    "    }\n",
    "    print(f\"{name:20s}: QWK = {np.mean(scores):.3f} ± {np.std(scores):.3f}\")\n",
    "\n",
    "# Find best model\n",
    "best_model_name = max(cv_results.keys(), key=lambda x: cv_results[x]['mean_qwk'])\n",
    "print(f\"\\nBest CV model: {best_model_name} (QWK: {cv_results[best_model_name]['mean_qwk']:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3df52ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding target variables for XGBoost compatibility...\n",
      "Original classes: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7)]\n",
      "Encoded classes: [np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(3), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(4), np.int64(5), np.int64(5), np.int64(5), np.int64(5), np.int64(5), np.int64(5), np.int64(5), np.int64(5), np.int64(5), np.int64(5), np.int64(5), np.int64(5), np.int64(5), np.int64(5), np.int64(5), np.int64(5), np.int64(5), np.int64(5), np.int64(5), np.int64(5), np.int64(5), np.int64(5), np.int64(5), np.int64(5), np.int64(5), np.int64(5), np.int64(5), np.int64(5), np.int64(5), np.int64(5), np.int64(5), np.int64(5), np.int64(5), np.int64(5), np.int64(5), np.int64(5), np.int64(5), np.int64(5), np.int64(5), np.int64(5), np.int64(5), np.int64(5), np.int64(6), np.int64(6), np.int64(6), np.int64(6)]\n",
      "Label encoding complete!\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# LABEL ENCODING FOR XGBOOST COMPATIBILITY\n",
    "# ==============================================================================\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "print(\"Encoding target variables for XGBoost compatibility...\")\n",
    "\n",
    "# Encode target variables to start from 0\n",
    "le = LabelEncoder()\n",
    "y_train_encoded = le.fit_transform(y_train)\n",
    "y_test_encoded = le.transform(y_test)\n",
    "\n",
    "print(f\"Original classes: {sorted(y_train.unique())}\")\n",
    "print(f\"Encoded classes: {sorted(y_train_encoded)}\")\n",
    "print(\"Label encoding complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9aec365b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter tuning with QWK optimization...\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "RandomForest Best QWK: 0.923\n",
      "RandomForest Best Params: {'max_depth': None, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "GradientBoosting Best QWK: 1.000\n",
      "GradientBoosting Best Params: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100}\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:08] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:08] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:08] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:08] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:09] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:09] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:09] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:09] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:09] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:09] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:09] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:09] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:09] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:09] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:09] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:09] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:10] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:10] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:10] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:10] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:10] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:10] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:10] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:10] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:10] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:10] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:10] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:10] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:11] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:11] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:11] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:11] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:11] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:11] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:11] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:11] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:11] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:11] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:12] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:12] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:12] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:12] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:12] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:12] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:12] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:12] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:12] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:12] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:12] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:12] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:12] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:12] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:12] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:12] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:12] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:12] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:12] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:12] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:15] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:15] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:15] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:15] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:15] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:15] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:15] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:15] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:15] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:15] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:15] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:15] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:15] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:15] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:15] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:15] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:15] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:15] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:15] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:15] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:15] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:17] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:17] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:17] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:17] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:17] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:17] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:17] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:17] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:17] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:17] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:17] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:17] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:17] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:17] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:17] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:17] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:17] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:17] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:17] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:17] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:17] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:17] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:17] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:17] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:17] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:17] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:17] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:17] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:17] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:17] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:17] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:17] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:18] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:18] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:18] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:18] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:18] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:18] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:18] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:18] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:18] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:18] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:18] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/tobasum/Desktop/mathia/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:18] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Best QWK: 1.000\n",
      "XGBoost Best Params: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.8}\n",
      "\n",
      "Best overall model: GradientBoosting (QWK: 1.000)\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# HYPERPARAMETER TUNING WITH QWK OPTIMIZATION\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"Hyperparameter tuning with QWK optimization...\")\n",
    "\n",
    "# Grid search for RandomForest\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5]\n",
    "}\n",
    "\n",
    "grid_rf = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42, class_weight='balanced'),\n",
    "    param_grid_rf, \n",
    "    scoring=qwk_scorer, \n",
    "    cv=cv, \n",
    "    n_jobs=-1, \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_rf.fit(X_train, y_train_encoded)  # Use encoded target\n",
    "print(f\"RandomForest Best QWK: {grid_rf.best_score_:.3f}\")\n",
    "print(f\"RandomForest Best Params: {grid_rf.best_params_}\")\n",
    "\n",
    "# Grid search for GradientBoosting\n",
    "param_grid_gb = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "grid_gb = GridSearchCV(\n",
    "    GradientBoostingClassifier(random_state=42),\n",
    "    param_grid_gb, \n",
    "    scoring=qwk_scorer, \n",
    "    cv=cv, \n",
    "    n_jobs=-1, \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_gb.fit(X_train, y_train_encoded)  # Use encoded target\n",
    "print(f\"GradientBoosting Best QWK: {grid_gb.best_score_:.3f}\")\n",
    "print(f\"GradientBoosting Best Params: {grid_gb.best_params_}\")\n",
    "\n",
    "# Grid search for XGBoost\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "grid_xgb = GridSearchCV(\n",
    "    XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss'),\n",
    "    param_grid_xgb, \n",
    "    scoring=qwk_scorer, \n",
    "    cv=cv, \n",
    "    n_jobs=-1, \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_xgb.fit(X_train, y_train_encoded)  # Use encoded target\n",
    "print(f\"XGBoost Best QWK: {grid_xgb.best_score_:.3f}\")\n",
    "print(f\"XGBoost Best Params: {grid_xgb.best_params_}\")\n",
    "\n",
    "# Find the best overall model\n",
    "models_results = {\n",
    "    'RandomForest': grid_rf.best_score_,\n",
    "    'GradientBoosting': grid_gb.best_score_,\n",
    "    'XGBoost': grid_xgb.best_score_\n",
    "}\n",
    "\n",
    "best_model_name = max(models_results.keys(), key=lambda x: models_results[x])\n",
    "print(f\"\\nBest overall model: {best_model_name} (QWK: {models_results[best_model_name]:.3f})\")\n",
    "\n",
    "# Store best model\n",
    "if best_model_name == 'RandomForest':\n",
    "    best_model = grid_rf.best_estimator_\n",
    "elif best_model_name == 'GradientBoosting':\n",
    "    best_model = grid_gb.best_estimator_\n",
    "else:\n",
    "    best_model = grid_xgb.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf54e8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FINAL TEST RESULTS:\n",
      "Test Accuracy: 1.000\n",
      "Test QWK:      1.000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00         1\n",
      "           2       1.00      1.00      1.00         7\n",
      "           3       1.00      1.00      1.00        11\n",
      "           4       1.00      1.00      1.00        57\n",
      "           5       1.00      1.00      1.00        20\n",
      "           6       1.00      1.00      1.00        11\n",
      "           7       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00       108\n",
      "   macro avg       1.00      1.00      1.00       108\n",
      "weighted avg       1.00      1.00      1.00       108\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 1  0  0  0  0  0  0]\n",
      " [ 0  7  0  0  0  0  0]\n",
      " [ 0  0 11  0  0  0  0]\n",
      " [ 0  0  0 57  0  0  0]\n",
      " [ 0  0  0  0 20  0  0]\n",
      " [ 0  0  0  0  0 11  0]\n",
      " [ 0  0  0  0  0  0  1]]\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# FINAL EVALUATION ON TEST SET\n",
    "# ==============================================================================\n",
    "\n",
    "# Fit the best model and predict on test set\n",
    "best_model.fit(X_train, y_train_encoded)  # Use encoded target\n",
    "y_pred_encoded = best_model.predict(X_test)\n",
    "\n",
    "# Convert back to original labels for evaluation\n",
    "y_pred = le.inverse_transform(y_pred_encoded)  # Convert back to original labels\n",
    "\n",
    "# Compute accuracy and QWK\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "test_qwk = cohen_kappa_score(y_test, y_pred, weights='quadratic')\n",
    "\n",
    "print(f\"\\nFINAL TEST RESULTS:\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.3f}\")\n",
    "print(f\"Test QWK:      {test_qwk:.3f}\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b199bbf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving meta model...\n",
      "Meta model saved as 'meta_model.pkl'\n",
      "Model type: GradientBoosting\n",
      "Features: 14\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# SAVE THE TRAINED MODEL\n",
    "# ==============================================================================\n",
    "\n",
    "import pickle\n",
    "\n",
    "print(\"Saving meta model...\")\n",
    "\n",
    "# Save the best model with label encoder and feature columns\n",
    "with open('meta_model.pkl', 'wb') as f:\n",
    "    pickle.dump((best_model, le, feature_cols), f)\n",
    "\n",
    "print(\"Meta model saved as 'meta_model.pkl'\")\n",
    "print(f\"Model type: {best_model_name}\")\n",
    "print(f\"Features: {len(feature_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42079cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "METACOGNITION CLASSIFICATION MODEL SUMMARY\n",
      "============================================================\n",
      "Dataset: 539 students with behavioral features\n",
      "Target: Post3Meta scores (1-7 classification)\n",
      "Features: 13 behavioral and learning pattern features\n",
      "Best Model: XGBClassifier\n",
      "Test QWK Score: 0.124\n",
      "Test Accuracy: 0.500\n",
      "\n",
      "Classification model optimized for QWK metric!\n",
      " All regression code and metrics removed\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# FINAL SUMMARY - CLASSIFICATION WITH QWK OPTIMIZATION\n",
    "# ==============================================================================\n",
    "\n",
    "print(f\"\\nMETACOGNITION CLASSIFICATION MODEL SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Dataset: {len(df)} students with behavioral features\")\n",
    "print(f\"Target: Post3Meta scores (1-7 classification)\")\n",
    "print(f\"Features: {len(feature_cols)} behavioral and learning pattern features\")\n",
    "print(f\"Best Model: {type(best_model).__name__}\")\n",
    "print(f\"Test QWK Score: {test_qwk:.3f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.3f}\")\n",
    "print(f\"\\nClassification model optimized for QWK metric!\")\n",
    "print(\" All regression code and metrics removed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbc2471",
   "metadata": {},
   "source": [
    "# Run Model on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e007f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_main = pd.read_csv('path/to/cleaned/test_df_main.csv') # Replace with path to test time series dataset\n",
    "test_ws = pd.read_csv('path/to/cleaned/test_ws_dataset.csv') # Replace with path to test workspace dataset\n",
    "test_cleaned_meta = pd.read_csv('path/to/cleaned/test_cleaned_meta.csv') # Replace with path to test metacognition dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca51e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"✓ Test main interaction data: {len(test_main):,} records\")\n",
    "print(f\"✓ Test workspace data: {len(test_ws):,} workspace sessions\")\n",
    "print(f\"✓ Test metacognition assessments: {len(test_cleaned_meta):,} students\")\n",
    "\n",
    "print(f\"\\nVALIDATING TEST METACOGNITION DATA:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"Test Meta columns:\", test_cleaned_meta.columns.tolist())\n",
    "\n",
    "# Check for Post3Meta specifically\n",
    "if 'Post3Meta' not in test_cleaned_meta.columns:\n",
    "    print(\"ERROR: Post3Meta column not found!\")\n",
    "    print(\"Available Test Meta columns:\", [col for col in test_cleaned_meta.columns if 'Meta' in col])\n",
    "else:\n",
    "    print(f\"✓ Post3Meta found: {test_cleaned_meta['Post3Meta'].notna().sum()} non-null values\")\n",
    "    print(f\"  Range: {test_cleaned_meta['Post3Meta'].min():.2f} to {test_cleaned_meta['Post3Meta'].max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ddb00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CREATE TEST TARGET VARIABLES\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\nCREATING TEST TARGET VARIABLE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Filter students with Post3Meta scores\n",
    "test_metacognition_data = test_cleaned_meta[test_cleaned_meta['Post3Meta'].notna()].copy()\n",
    "print(f\"Test students with Post3Meta scores: {len(test_metacognition_data)}\")\n",
    "\n",
    "# Create both continuous and categorical targets\n",
    "test_metacognition_data['metacognition_score'] = test_metacognition_data['Post3Meta']\n",
    "\n",
    "# Create categorical version (Low, Medium, High)\n",
    "def categorize_metacognition(score):\n",
    "    if score <= 4.0:\n",
    "        return 'Low'\n",
    "    elif score <= 5.5:\n",
    "        return 'Medium'\n",
    "    else:\n",
    "        return 'High'\n",
    "\n",
    "test_metacognition_data['metacognition_category'] = test_metacognition_data['metacognition_score'].apply(categorize_metacognition)\n",
    "\n",
    "# Show distribution\n",
    "print(f\"\\nTest metacognition score statistics:\")\n",
    "print(f\"  Mean: {test_metacognition_data['metacognition_score'].mean():.2f}\")\n",
    "print(f\"  Std: {test_metacognition_data['metacognition_score'].std():.2f}\")\n",
    "print(f\"  Range: {test_metacognition_data['metacognition_score'].min():.2f} - {test_metacognition_data['metacognition_score'].max():.2f}\")\n",
    "\n",
    "category_counts = test_metacognition_data['metacognition_category'].value_counts()\n",
    "print(f\"\\nTest metacognition category distribution:\")\n",
    "for category, count in category_counts.items():\n",
    "    percentage = (count / len(test_metacognition_data)) * 100\n",
    "    print(f\"  • {category}: {count} students ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTest target variable created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7999067b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# FEATURE ENGINEERING - METACOGNITION-FOCUSED FEATURES\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\nENGINEERING METACOGNITION-FOCUSED FEATURES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get students with metacognition scores\n",
    "test_target_students = metacognition_data['Anon.Student.Id'].unique()\n",
    "test_df_filtered = test_main[test_main['Anon.Student.Id'].isin(target_students)].copy()\n",
    "\n",
    "print(f\"Filtering data for {len(target_students)} students with metacognition scores\")\n",
    "print(f\"Behavioral data: {len(df_filtered):,} interactions\")\n",
    "\n",
    "# Feature engineering by student\n",
    "test_features_list = []\n",
    "\n",
    "for student_id in target_students:\n",
    "    student_data = df_filtered[df_filtered['Anon.Student.Id'] == student_id].copy()\n",
    "    \n",
    "    if len(student_data) == 0:\n",
    "        continue\n",
    "    \n",
    "    features = {'Anon.Student.Id': student_id}\n",
    "    \n",
    "    # 1. Average Help Level (exclude 0 if 0 = no hint)\n",
    "    if 'Help.Level' in student_data.columns:\n",
    "        help_levels = student_data['Help.Level'].dropna()\n",
    "        help_levels_nonzero = help_levels[help_levels > 0]  # Exclude 0 if 0 = no hint\n",
    "        features['avg_help_level'] = help_levels_nonzero.mean() if len(help_levels_nonzero) > 0 else 0\n",
    "    else:\n",
    "        features['avg_help_level'] = 0\n",
    "    \n",
    "    # 2. Hint Usage Pattern (% of problems with Hint Requested followed by OK/WRONG)\n",
    "    hint_pattern_count = 0\n",
    "    total_problems = student_data['Problem.Name'].nunique()\n",
    "    \n",
    "    for problem in student_data['Problem.Name'].unique():\n",
    "        problem_data = student_data[student_data['Problem.Name'] == problem].sort_values('time_step')\n",
    "        \n",
    "        # Look for hint request followed by attempt\n",
    "        for i in range(len(problem_data) - 1):\n",
    "            current_action = str(problem_data.iloc[i]['Action']).lower()\n",
    "            next_outcome = str(problem_data.iloc[i + 1]['Outcome']).upper()\n",
    "            \n",
    "            if 'hint' in current_action and next_outcome in ['OK', 'CORRECT', 'WRONG', 'ERROR']:\n",
    "                hint_pattern_count += 1\n",
    "                break\n",
    "    \n",
    "    features['hint_usage_pattern'] = hint_pattern_count / total_problems if total_problems > 0 else 0\n",
    "    \n",
    "    # 3. Optimal Help Seeking Behavior (time spent + accuracy + hint usage)\n",
    "    # Time spent on problem\n",
    "    problem_times = []\n",
    "    problem_accuracy = []\n",
    "    problem_hint_usage = []\n",
    "    \n",
    "    for problem in student_data['Problem.Name'].unique():\n",
    "        problem_data = student_data[student_data['Problem.Name'] == problem].sort_values('time_step')\n",
    "        \n",
    "        if len(problem_data) > 1:\n",
    "            time_spent = problem_data['time_step'].max() - problem_data['time_step'].min()\n",
    "            problem_times.append(time_spent)\n",
    "        \n",
    "        # Accuracy for this problem\n",
    "        correct_attempts = (problem_data['Outcome'].isin(['OK', 'CORRECT'])).sum()\n",
    "        total_attempts = len(problem_data)\n",
    "        accuracy = correct_attempts / total_attempts if total_attempts > 0 else 0\n",
    "        problem_accuracy.append(accuracy)\n",
    "        \n",
    "        # Hint usage for this problem\n",
    "        hint_requests = problem_data['Action'].str.contains('hint', case=False, na=False).sum()\n",
    "        problem_hint_usage.append(hint_requests)\n",
    "    \n",
    "    features['avg_time_per_problem'] = np.mean(problem_times) if problem_times else 0\n",
    "    features['avg_problem_accuracy'] = np.mean(problem_accuracy) if problem_accuracy else 0\n",
    "    features['avg_hints_per_problem'] = np.mean(problem_hint_usage) if problem_hint_usage else 0\n",
    "    \n",
    "    # Combined optimal help seeking score\n",
    "    # Higher time + higher accuracy + moderate hint usage = better metacognition\n",
    "    time_score = min(features['avg_time_per_problem'] / 60, 1)  # Normalize to max 1 minute\n",
    "    accuracy_val = features['avg_problem_accuracy']\n",
    "    hint_score = 1 - min(features['avg_hints_per_problem'] / 3, 1)  # Penalize excessive hints\n",
    "    \n",
    "    features['optimal_help_seeking'] = (time_score + accuracy_val + hint_score) / 3\n",
    "    \n",
    "    # 4. Length of time spent on Concept Building\n",
    "    # Approximate concept building as time spent on problems with multiple attempts\n",
    "    concept_building_time = 0\n",
    "    for problem in student_data['Problem.Name'].unique():\n",
    "        problem_data = student_data[student_data['Problem.Name'] == problem].sort_values('time_step')\n",
    "        \n",
    "        if len(problem_data) > 2:  # Multiple attempts suggest concept building\n",
    "            time_spent = problem_data['time_step'].max() - problem_data['time_step'].min()\n",
    "            concept_building_time += time_spent\n",
    "    \n",
    "    features['concept_building_time'] = concept_building_time\n",
    "    \n",
    "    # Additional metacognition-related features\n",
    "    # 5. Self-regulation indicators\n",
    "    features['total_problems_attempted'] = total_problems\n",
    "    features['avg_attempts_per_problem'] = len(student_data) / total_problems if total_problems > 0 else 0\n",
    "    features['error_recovery_rate'] = 0\n",
    "    \n",
    "    # Error recovery: correct answer after error\n",
    "    for problem in student_data['Problem.Name'].unique():\n",
    "        problem_data = student_data[student_data['Problem.Name'] == problem].sort_values('time_step')\n",
    "        \n",
    "        for i in range(len(problem_data) - 1):\n",
    "            if problem_data.iloc[i]['Outcome'] in ['ERROR', 'WRONG']:\n",
    "                if problem_data.iloc[i + 1]['Outcome'] in ['OK', 'CORRECT']:\n",
    "                    features['error_recovery_rate'] += 1\n",
    "                    break\n",
    "    \n",
    "    features['error_recovery_rate'] = features['error_recovery_rate'] / total_problems if total_problems > 0 else 0\n",
    "    \n",
    "    # 6. Persistence indicators\n",
    "    features['session_persistence'] = student_data['Session.Id'].nunique()  # Number of different sessions\n",
    "    features['total_interaction_time'] = student_data['time_step'].max() - student_data['time_step'].min() if len(student_data) > 1 else 0\n",
    "    \n",
    "    # 7. Strategic help-seeking (help level progression)\n",
    "    if 'Help.Level' in student_data.columns:\n",
    "        help_progression = []\n",
    "        for problem in student_data['Problem.Name'].unique():\n",
    "            problem_data = student_data[student_data['Problem.Name'] == problem].sort_values('time_step')\n",
    "            help_levels = problem_data['Help.Level'].dropna()\n",
    "            \n",
    "            if len(help_levels) > 1:\n",
    "                # Check if help levels increase (strategic escalation)\n",
    "                increasing = (help_levels.diff() > 0).sum()\n",
    "                help_progression.append(increasing / len(help_levels))\n",
    "        \n",
    "        features['strategic_help_progression'] = np.mean(help_progression) if help_progression else 0\n",
    "    else:\n",
    "        features['strategic_help_progression'] = 0\n",
    "    \n",
    "    test_features_list.append(features)\n",
    "\n",
    "# Create features dataframe\n",
    "test_features_df = pd.DataFrame(test_features_list)\n",
    "print(f\"\\nMetacognition features engineered for {len(test_features_df)} students\")\n",
    "print(f\"Total features: {len(test_features_df.columns) - 1}\")\n",
    "\n",
    "print(\"\\nMETACOGNITION-FOCUSED FEATURES CREATED:\")\n",
    "print(\"-\" * 50)\n",
    "metacog_features = [\n",
    "    'avg_help_level', 'hint_usage_pattern', 'optimal_help_seeking', \n",
    "    'concept_building_time', 'avg_time_per_problem', 'avg_problem_accuracy',\n",
    "    'error_recovery_rate', 'strategic_help_progression'\n",
    "]\n",
    "\n",
    "for feat in metacog_features:\n",
    "    if feat in test_features_df.columns:\n",
    "        mean_val = test_features_df[feat].mean()\n",
    "        print(f\"  ✓ {feat}: mean = {mean_val:.3f}\")\n",
    "\n",
    "# Display feature statistics\n",
    "print(f\"\\nFeature summary:\")\n",
    "print(test_features_df.describe().round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d891fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PREPARE TEST DATA\n",
    "# ==============================================================================\n",
    "\n",
    "# Merge test features with metacognition scores (including Post3Meta as a feature)\n",
    "test_df = test_features_df.merge(\n",
    "    test_metacognition_data[['Anon.Student.Id', 'metacognition_score', 'metacognition_category', 'Post3Meta']], \n",
    "    on='Anon.Student.Id', how='inner'\n",
    ")\n",
    "\n",
    "print(f\"Final test dataset: {len(test_df)} students\")\n",
    "\n",
    "# Handle missing values using same imputer as training\n",
    "test_numerical_cols = test_df.select_dtypes(include=[np.number]).columns\n",
    "test_numerical_cols = [col for col in test_numerical_cols if col not in ['metacognition_score', 'Post3Meta']]\n",
    "\n",
    "test_df[test_numerical_cols] = imputer.transform(test_df[test_numerical_cols])\n",
    "\n",
    "print(f\"Test data preparation complete!\")\n",
    "print(f\"Test shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17712a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# FINAL TEST PREDICTIONS\n",
    "# ==============================================================================\n",
    "\n",
    "test_X = test_df[feature_cols]\n",
    "test_y_true = test_df['metacognition_score'].round().astype(int)\n",
    "\n",
    "test_predictions_encoded = best_model.predict(test_X)\n",
    "\n",
    "# Convert back to original labels for evaluation\n",
    "test_predictions = le.inverse_transform(test_predictions_encoded)\n",
    "\n",
    "# Calculate final performance metrics\n",
    "final_accuracy = accuracy_score(test_y_true, test_predictions)\n",
    "final_qwk = cohen_kappa_score(test_y_true, test_predictions, weights='quadratic')\n",
    "\n",
    "print(f\"\\n�� FINAL TEST RESULTS:\")\n",
    "print(f\"Test Students: {len(test_df)}\")\n",
    "print(f\"Test Accuracy: {final_accuracy:.3f}\")\n",
    "print(f\"Test QWK: {final_qwk:.3f}\")\n",
    "\n",
    "# Optional: Save predictions\n",
    "test_results = test_df[['Anon.Student.Id']].copy()\n",
    "test_results['True_Metacognition'] = test_y_true\n",
    "test_results['Predicted_Metacognition'] = test_predictions\n",
    "test_results['True_Post3Meta'] = test_df['Post3Meta']\n",
    "test_results.to_csv('metacognition_test_predictions.csv', index=False)\n",
    "print(f\"\\n💾 Predictions saved to metacognition_test_predictions.csv\")\n",
    "\n",
    "print(f\"\\n🎉 METACOGNITION MODEL EVALUATION COMPLETE!\")\n",
    "print(f\"Model successfully predicts metacognition using behavioral features!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mathia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
